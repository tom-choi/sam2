{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2最新版训练Unet全流程\n",
    "    包含 random随机种子存储，大图预处理(包含旋转，缩放，色彩对比度范围随机变化，图像填充裁切)，patch分割，训练，存储model，测试model。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载基本库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# 封装的函数模块，model部分全部引用\n",
    "from functions.data import prepare_dataset,get_optimized_loaders\n",
    "from functions.model import *\n",
    "from functions.data import SegmentationDataset\n",
    "from functions.imagePreprocessing import ImagePreprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data (Lucchi++): [{'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0000.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\0.png', 'index': 0}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0001.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\1.png', 'index': 1}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0002.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\2.png', 'index': 2}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0003.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\3.png', 'index': 3}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0004.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\4.png', 'index': 4}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0005.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\5.png', 'index': 5}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0006.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\6.png', 'index': 6}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0007.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\7.png', 'index': 7}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0008.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\8.png', 'index': 8}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0009.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\9.png', 'index': 9}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0010.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\10.png', 'index': 10}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0011.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\11.png', 'index': 11}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0012.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\12.png', 'index': 12}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0013.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\13.png', 'index': 13}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0014.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\14.png', 'index': 14}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0015.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\15.png', 'index': 15}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0016.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\16.png', 'index': 16}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0017.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\17.png', 'index': 17}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0018.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\18.png', 'index': 18}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0019.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\19.png', 'index': 19}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0020.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\20.png', 'index': 20}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0021.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\21.png', 'index': 21}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0022.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\22.png', 'index': 22}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0023.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\23.png', 'index': 23}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0024.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\24.png', 'index': 24}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0025.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\25.png', 'index': 25}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0026.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\26.png', 'index': 26}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0027.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\27.png', 'index': 27}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0028.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\28.png', 'index': 28}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0029.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\29.png', 'index': 29}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0030.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\30.png', 'index': 30}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0031.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\31.png', 'index': 31}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0032.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\32.png', 'index': 32}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0033.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\33.png', 'index': 33}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0034.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\34.png', 'index': 34}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0035.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\35.png', 'index': 35}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0036.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\36.png', 'index': 36}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0037.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\37.png', 'index': 37}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0038.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\38.png', 'index': 38}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0039.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\39.png', 'index': 39}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0040.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\40.png', 'index': 40}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0041.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\41.png', 'index': 41}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0042.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\42.png', 'index': 42}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0043.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\43.png', 'index': 43}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0044.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\44.png', 'index': 44}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0045.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\45.png', 'index': 45}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0046.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\46.png', 'index': 46}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0047.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\47.png', 'index': 47}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0048.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\48.png', 'index': 48}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0049.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\49.png', 'index': 49}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0050.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\50.png', 'index': 50}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0051.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\51.png', 'index': 51}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0052.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\52.png', 'index': 52}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0053.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\53.png', 'index': 53}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0054.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\54.png', 'index': 54}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0055.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\55.png', 'index': 55}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0056.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\56.png', 'index': 56}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0057.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\57.png', 'index': 57}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0058.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\58.png', 'index': 58}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0059.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\59.png', 'index': 59}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0060.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\60.png', 'index': 60}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0061.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\61.png', 'index': 61}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0062.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\62.png', 'index': 62}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0063.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\63.png', 'index': 63}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0064.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\64.png', 'index': 64}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0065.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\65.png', 'index': 65}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0066.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\66.png', 'index': 66}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0067.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\67.png', 'index': 67}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0068.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\68.png', 'index': 68}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0069.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\69.png', 'index': 69}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0070.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\70.png', 'index': 70}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0071.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\71.png', 'index': 71}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0072.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\72.png', 'index': 72}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0073.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\73.png', 'index': 73}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0074.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\74.png', 'index': 74}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0075.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\75.png', 'index': 75}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0076.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\76.png', 'index': 76}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0077.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\77.png', 'index': 77}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0078.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\78.png', 'index': 78}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0079.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\79.png', 'index': 79}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0080.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\80.png', 'index': 80}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0081.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\81.png', 'index': 81}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0082.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\82.png', 'index': 82}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0083.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\83.png', 'index': 83}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0084.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\84.png', 'index': 84}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0085.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\85.png', 'index': 85}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0086.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\86.png', 'index': 86}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0087.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\87.png', 'index': 87}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0088.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\88.png', 'index': 88}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0089.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\89.png', 'index': 89}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0090.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\90.png', 'index': 90}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0091.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\91.png', 'index': 91}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0092.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\92.png', 'index': 92}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0093.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\93.png', 'index': 93}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0094.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\94.png', 'index': 94}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0095.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\95.png', 'index': 95}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0096.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\96.png', 'index': 96}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0097.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\97.png', 'index': 97}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0098.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\98.png', 'index': 98}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0099.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\99.png', 'index': 99}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0100.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\100.png', 'index': 100}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0101.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\101.png', 'index': 101}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0102.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\102.png', 'index': 102}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0103.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\103.png', 'index': 103}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0104.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\104.png', 'index': 104}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0105.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\105.png', 'index': 105}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0106.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\106.png', 'index': 106}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0107.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\107.png', 'index': 107}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0108.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\108.png', 'index': 108}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0109.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\109.png', 'index': 109}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0110.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\110.png', 'index': 110}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0111.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\111.png', 'index': 111}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0112.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\112.png', 'index': 112}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0113.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\113.png', 'index': 113}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0114.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\114.png', 'index': 114}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0115.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\115.png', 'index': 115}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0116.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\116.png', 'index': 116}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0117.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\117.png', 'index': 117}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0118.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\118.png', 'index': 118}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0119.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\119.png', 'index': 119}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0120.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\120.png', 'index': 120}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0121.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\121.png', 'index': 121}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0122.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\122.png', 'index': 122}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0123.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\123.png', 'index': 123}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0124.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\124.png', 'index': 124}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0125.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\125.png', 'index': 125}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0126.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\126.png', 'index': 126}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0127.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\127.png', 'index': 127}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0128.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\128.png', 'index': 128}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0129.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\129.png', 'index': 129}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0130.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\130.png', 'index': 130}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0131.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\131.png', 'index': 131}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0132.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\132.png', 'index': 132}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0133.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\133.png', 'index': 133}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0134.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\134.png', 'index': 134}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0135.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\135.png', 'index': 135}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0136.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\136.png', 'index': 136}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0137.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\137.png', 'index': 137}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0138.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\138.png', 'index': 138}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0139.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\139.png', 'index': 139}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0140.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\140.png', 'index': 140}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0141.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\141.png', 'index': 141}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0142.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\142.png', 'index': 142}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0143.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\143.png', 'index': 143}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0144.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\144.png', 'index': 144}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0145.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\145.png', 'index': 145}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0146.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\146.png', 'index': 146}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0147.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\147.png', 'index': 147}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0148.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\148.png', 'index': 148}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0149.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\149.png', 'index': 149}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0150.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\150.png', 'index': 150}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0151.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\151.png', 'index': 151}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0152.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\152.png', 'index': 152}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0153.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\153.png', 'index': 153}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0154.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\154.png', 'index': 154}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0155.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\155.png', 'index': 155}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0156.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\156.png', 'index': 156}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0157.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\157.png', 'index': 157}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0158.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\158.png', 'index': 158}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0159.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\159.png', 'index': 159}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0160.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\160.png', 'index': 160}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0161.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\161.png', 'index': 161}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0162.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\162.png', 'index': 162}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0163.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\163.png', 'index': 163}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0164.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\164.png', 'index': 164}]\n",
      "Test Data (Lucchi++): [{'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0000.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\0.png', 'index': 0}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0001.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\1.png', 'index': 1}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0002.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\2.png', 'index': 2}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0003.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\3.png', 'index': 3}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0004.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\4.png', 'index': 4}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0005.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\5.png', 'index': 5}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0006.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\6.png', 'index': 6}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0007.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\7.png', 'index': 7}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0008.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\8.png', 'index': 8}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0009.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\9.png', 'index': 9}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0010.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\10.png', 'index': 10}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0011.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\11.png', 'index': 11}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0012.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\12.png', 'index': 12}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0013.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\13.png', 'index': 13}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0014.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\14.png', 'index': 14}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0015.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\15.png', 'index': 15}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0016.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\16.png', 'index': 16}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0017.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\17.png', 'index': 17}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0018.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\18.png', 'index': 18}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0019.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\19.png', 'index': 19}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0020.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\20.png', 'index': 20}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0021.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\21.png', 'index': 21}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0022.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\22.png', 'index': 22}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0023.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\23.png', 'index': 23}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0024.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\24.png', 'index': 24}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0025.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\25.png', 'index': 25}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0026.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\26.png', 'index': 26}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0027.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\27.png', 'index': 27}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0028.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\28.png', 'index': 28}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0029.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\29.png', 'index': 29}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0030.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\30.png', 'index': 30}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0031.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\31.png', 'index': 31}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0032.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\32.png', 'index': 32}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0033.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\33.png', 'index': 33}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0034.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\34.png', 'index': 34}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0035.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\35.png', 'index': 35}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0036.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\36.png', 'index': 36}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0037.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\37.png', 'index': 37}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0038.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\38.png', 'index': 38}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0039.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\39.png', 'index': 39}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0040.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\40.png', 'index': 40}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0041.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\41.png', 'index': 41}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0042.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\42.png', 'index': 42}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0043.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\43.png', 'index': 43}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0044.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\44.png', 'index': 44}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0045.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\45.png', 'index': 45}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0046.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\46.png', 'index': 46}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0047.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\47.png', 'index': 47}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0048.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\48.png', 'index': 48}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0049.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\49.png', 'index': 49}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0050.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\50.png', 'index': 50}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0051.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\51.png', 'index': 51}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0052.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\52.png', 'index': 52}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0053.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\53.png', 'index': 53}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0054.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\54.png', 'index': 54}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0055.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\55.png', 'index': 55}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0056.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\56.png', 'index': 56}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0057.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\57.png', 'index': 57}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0058.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\58.png', 'index': 58}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0059.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\59.png', 'index': 59}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0060.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\60.png', 'index': 60}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0061.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\61.png', 'index': 61}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0062.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\62.png', 'index': 62}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0063.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\63.png', 'index': 63}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0064.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\64.png', 'index': 64}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0065.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\65.png', 'index': 65}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0066.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\66.png', 'index': 66}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0067.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\67.png', 'index': 67}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0068.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\68.png', 'index': 68}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0069.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\69.png', 'index': 69}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0070.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\70.png', 'index': 70}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0071.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\71.png', 'index': 71}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0072.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\72.png', 'index': 72}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0073.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\73.png', 'index': 73}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0074.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\74.png', 'index': 74}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0075.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\75.png', 'index': 75}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0076.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\76.png', 'index': 76}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0077.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\77.png', 'index': 77}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0078.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\78.png', 'index': 78}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0079.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\79.png', 'index': 79}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0080.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\80.png', 'index': 80}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0081.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\81.png', 'index': 81}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0082.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\82.png', 'index': 82}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0083.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\83.png', 'index': 83}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0084.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\84.png', 'index': 84}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0085.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\85.png', 'index': 85}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0086.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\86.png', 'index': 86}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0087.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\87.png', 'index': 87}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0088.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\88.png', 'index': 88}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0089.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\89.png', 'index': 89}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0090.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\90.png', 'index': 90}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0091.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\91.png', 'index': 91}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0092.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\92.png', 'index': 92}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0093.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\93.png', 'index': 93}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0094.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\94.png', 'index': 94}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0095.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\95.png', 'index': 95}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0096.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\96.png', 'index': 96}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0097.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\97.png', 'index': 97}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0098.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\98.png', 'index': 98}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0099.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\99.png', 'index': 99}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0100.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\100.png', 'index': 100}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0101.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\101.png', 'index': 101}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0102.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\102.png', 'index': 102}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0103.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\103.png', 'index': 103}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0104.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\104.png', 'index': 104}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0105.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\105.png', 'index': 105}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0106.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\106.png', 'index': 106}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0107.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\107.png', 'index': 107}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0108.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\108.png', 'index': 108}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0109.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\109.png', 'index': 109}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0110.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\110.png', 'index': 110}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0111.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\111.png', 'index': 111}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0112.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\112.png', 'index': 112}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0113.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\113.png', 'index': 113}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0114.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\114.png', 'index': 114}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0115.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\115.png', 'index': 115}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0116.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\116.png', 'index': 116}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0117.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\117.png', 'index': 117}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0118.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\118.png', 'index': 118}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0119.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\119.png', 'index': 119}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0120.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\120.png', 'index': 120}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0121.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\121.png', 'index': 121}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0122.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\122.png', 'index': 122}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0123.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\123.png', 'index': 123}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0124.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\124.png', 'index': 124}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0125.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\125.png', 'index': 125}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0126.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\126.png', 'index': 126}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0127.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\127.png', 'index': 127}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0128.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\128.png', 'index': 128}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0129.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\129.png', 'index': 129}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0130.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\130.png', 'index': 130}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0131.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\131.png', 'index': 131}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0132.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\132.png', 'index': 132}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0133.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\133.png', 'index': 133}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0134.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\134.png', 'index': 134}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0135.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\135.png', 'index': 135}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0136.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\136.png', 'index': 136}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0137.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\137.png', 'index': 137}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0138.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\138.png', 'index': 138}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0139.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\139.png', 'index': 139}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0140.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\140.png', 'index': 140}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0141.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\141.png', 'index': 141}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0142.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\142.png', 'index': 142}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0143.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\143.png', 'index': 143}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0144.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\144.png', 'index': 144}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0145.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\145.png', 'index': 145}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0146.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\146.png', 'index': 146}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0147.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\147.png', 'index': 147}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0148.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\148.png', 'index': 148}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0149.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\149.png', 'index': 149}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0150.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\150.png', 'index': 150}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0151.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\151.png', 'index': 151}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0152.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\152.png', 'index': 152}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0153.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\153.png', 'index': 153}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0154.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\154.png', 'index': 154}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0155.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\155.png', 'index': 155}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0156.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\156.png', 'index': 156}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0157.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\157.png', 'index': 157}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0158.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\158.png', 'index': 158}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0159.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\159.png', 'index': 159}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0160.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\160.png', 'index': 160}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0161.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\161.png', 'index': 161}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0162.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\162.png', 'index': 162}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0163.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\163.png', 'index': 163}, {'image': 'dataset\\\\Lucchi++\\\\Test_In\\\\mask0164.png', 'annotation': 'dataset\\\\Lucchi++\\\\Test_Out\\\\164.png', 'index': 164}]\n",
      "训练样本数量: 165\n",
      "测试样本数量: 165\n",
      "开始数据集预处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理图像: 100%|██████████| 165/165 [00:20<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集预处理完成，共生成 7865 个patch\n",
      "开始数据集预处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理图像: 100%|██████████| 165/165 [00:23<00:00,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集预处理完成，共生成 7707 个patch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_data, test_data = prepare_dataset(\"Kasthuri++\")\n",
    "train_data, test_data = prepare_dataset(\"Lucchi++\")\n",
    "\n",
    "# 第一次运行时：预处理并保存\n",
    "train_dataset = SegmentationDataset(\n",
    "    data_list=train_data,\n",
    "    patch_size=256,\n",
    "    stride=128,\n",
    "    preProcess=True,  # 启用预处理\n",
    "\n",
    ")\n",
    "\n",
    "test_dataset = SegmentationDataset(\n",
    "    data_list=test_data,\n",
    "    patch_size=256,\n",
    "    stride=128,\n",
    "    preProcess=True,  # 启用预处理\n",
    "\n",
    ")\n",
    "\n",
    "train_loader, test_loader = get_optimized_loaders(\n",
    "    train_dataset, \n",
    "    test_dataset, \n",
    "    batch_size=32,  # 增加批大小\n",
    "    num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练Unet模型阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = UNet(num_classes=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Att_YNet(nn.Module):\n",
    "    class AttentionBlock(nn.Module):\n",
    "        def __init__(self, F_g, F_l, F_int, batch_norm=False):\n",
    "            super(Att_YNet.AttentionBlock, self).__init__()\n",
    "            self.W_g = nn.Sequential(\n",
    "                nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "                nn.BatchNorm2d(F_int) if batch_norm else nn.Identity()\n",
    "            )\n",
    "\n",
    "            self.W_x = nn.Sequential(\n",
    "                nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "                nn.BatchNorm2d(F_int) if batch_norm else nn.Identity()\n",
    "            )\n",
    "\n",
    "            self.psi = nn.Sequential(\n",
    "                nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "                nn.BatchNorm2d(1) if batch_norm else nn.Identity(),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        def forward(self, g, x):\n",
    "            # Ensure g and x have the same spatial dimensions\n",
    "            if g.size(2) != x.size(2) or g.size(3) != x.size(3):\n",
    "                g = F.interpolate(g, size=(x.size(2), x.size(3)), \n",
    "                                  mode='bilinear', align_corners=True)\n",
    "                \n",
    "            g1 = self.W_g(g)\n",
    "            x1 = self.W_x(x)\n",
    "            psi = self.relu(g1 + x1)\n",
    "            psi = self.psi(psi)\n",
    "            return x * psi\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(Att_YNet, self).__init__()\n",
    "        \n",
    "        # Encoder - Same as UNet\n",
    "        self.encoder = nn.ModuleList([\n",
    "            self.conv_block(3, 32, stride=2),\n",
    "            self.conv_block(32, 64, stride=2),\n",
    "            self.conv_block(64, 128, stride=2),\n",
    "            self.conv_block(128, 256, stride=2)\n",
    "        ])\n",
    "        \n",
    "        # Decoder - Same as UNet\n",
    "        self.decoder = nn.ModuleList([\n",
    "            self.upconv_block(256, 128),\n",
    "            self.upconv_block(128, 64),\n",
    "            self.upconv_block(64, 32),\n",
    "            self.upconv_block(32, 32)\n",
    "        ])\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.final = nn.Conv2d(32, num_classes, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Set this to True if you want both outputs\n",
    "        self.return_reconstructed = False\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels, stride=1):\n",
    "        # Exactly as in UNet\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        # Exactly as in UNet\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def set_return_reconstructed(self, value):\n",
    "        \"\"\"Set whether to return the reconstructed image along with the mask\"\"\"\n",
    "        self.return_reconstructed = value\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = []\n",
    "        for i, encoder_layer in enumerate(self.encoder):\n",
    "            x = encoder_layer(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        # Store the bottleneck for the second decoder path\n",
    "        bottleneck = x\n",
    "        \n",
    "        # Decoder (just like UNet)\n",
    "        for i, decoder_layer in enumerate(self.decoder):\n",
    "            x = decoder_layer(x)\n",
    "            if i < len(self.decoder) - 1:\n",
    "                x = x + features[-i-2]  # Skip connection (exactly as in UNet)\n",
    "        \n",
    "        # Get mask output (same as UNet's final output)\n",
    "        mask = self.final(x)\n",
    "        \n",
    "        # For now, we're just replicating the UNet's behavior\n",
    "        # Later we can add the autoencoder path\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = Att_YNet(num_classes=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:\n",
      "Training - Loss: 0.2722, IoU: 0.4515, Dice: 0.5881\n",
      "Validation - Loss: 0.1169, IoU: 0.6526, Dice: 0.7841\n",
      "------------------------------------------------------------\n",
      "Epoch 2/50:\n",
      "Training - Loss: 0.0904, IoU: 0.7153, Dice: 0.8329\n",
      "Validation - Loss: 0.3013, IoU: 0.2829, Dice: 0.4229\n",
      "------------------------------------------------------------\n",
      "Epoch 3/50:\n",
      "Training - Loss: 0.0559, IoU: 0.7905, Dice: 0.8824\n",
      "Validation - Loss: 0.0556, IoU: 0.7298, Dice: 0.8403\n",
      "------------------------------------------------------------\n",
      "Epoch 4/50:\n",
      "Training - Loss: 0.0426, IoU: 0.8276, Dice: 0.9053\n",
      "Validation - Loss: 0.0547, IoU: 0.7327, Dice: 0.8422\n",
      "------------------------------------------------------------\n",
      "Epoch 5/50:\n",
      "Training - Loss: 0.0345, IoU: 0.8535, Dice: 0.9205\n",
      "Validation - Loss: 0.6809, IoU: 0.3060, Dice: 0.4508\n",
      "------------------------------------------------------------\n",
      "Epoch 6/50:\n",
      "Training - Loss: 0.0280, IoU: 0.8788, Dice: 0.9353\n",
      "Validation - Loss: 0.0449, IoU: 0.7641, Dice: 0.8634\n",
      "------------------------------------------------------------\n",
      "Epoch 7/50:\n",
      "Training - Loss: 0.0255, IoU: 0.8880, Dice: 0.9404\n",
      "Validation - Loss: 0.0505, IoU: 0.7366, Dice: 0.8442\n",
      "------------------------------------------------------------\n",
      "Epoch 8/50:\n",
      "Training - Loss: 0.0219, IoU: 0.9026, Dice: 0.9487\n",
      "Validation - Loss: 0.0489, IoU: 0.7606, Dice: 0.8607\n",
      "------------------------------------------------------------\n",
      "Epoch 9/50:\n",
      "Training - Loss: 0.0192, IoU: 0.9146, Dice: 0.9553\n",
      "Validation - Loss: 0.0479, IoU: 0.7650, Dice: 0.8638\n",
      "------------------------------------------------------------\n",
      "Epoch 10/50:\n",
      "Training - Loss: 0.0174, IoU: 0.9219, Dice: 0.9593\n",
      "Validation - Loss: 0.0477, IoU: 0.7697, Dice: 0.8664\n",
      "------------------------------------------------------------\n",
      "Epoch 11/50:\n",
      "Training - Loss: 0.0170, IoU: 0.9221, Dice: 0.9594\n",
      "Validation - Loss: 0.0473, IoU: 0.7746, Dice: 0.8700\n",
      "------------------------------------------------------------\n",
      "Epoch 12/50:\n",
      "Training - Loss: 0.0168, IoU: 0.9230, Dice: 0.9599\n",
      "Validation - Loss: 0.0524, IoU: 0.7621, Dice: 0.8620\n",
      "------------------------------------------------------------\n",
      "Epoch 13/50:\n",
      "Training - Loss: 0.0142, IoU: 0.9365, Dice: 0.9672\n",
      "Validation - Loss: 0.0483, IoU: 0.7772, Dice: 0.8719\n",
      "------------------------------------------------------------\n",
      "Epoch 14/50:\n",
      "Training - Loss: 0.0135, IoU: 0.9397, Dice: 0.9689\n",
      "Validation - Loss: 0.0489, IoU: 0.7795, Dice: 0.8733\n",
      "------------------------------------------------------------\n",
      "Epoch 15/50:\n",
      "Training - Loss: 0.0138, IoU: 0.9384, Dice: 0.9681\n",
      "Validation - Loss: 0.0591, IoU: 0.7366, Dice: 0.8440\n",
      "------------------------------------------------------------\n",
      "Epoch 16/50:\n",
      "Training - Loss: 0.0138, IoU: 0.9373, Dice: 0.9676\n",
      "Validation - Loss: 0.0506, IoU: 0.7773, Dice: 0.8719\n",
      "------------------------------------------------------------\n",
      "Epoch 17/50:\n",
      "Training - Loss: 0.0126, IoU: 0.9440, Dice: 0.9712\n",
      "Validation - Loss: 0.0522, IoU: 0.7800, Dice: 0.8735\n",
      "------------------------------------------------------------\n",
      "Epoch 18/50:\n",
      "Training - Loss: 0.0122, IoU: 0.9456, Dice: 0.9720\n",
      "Validation - Loss: 0.0528, IoU: 0.7742, Dice: 0.8700\n",
      "------------------------------------------------------------\n",
      "Epoch 19/50:\n",
      "Training - Loss: 0.0117, IoU: 0.9487, Dice: 0.9737\n",
      "Validation - Loss: 0.0519, IoU: 0.7783, Dice: 0.8726\n",
      "------------------------------------------------------------\n",
      "Epoch 20/50:\n",
      "Training - Loss: 0.0114, IoU: 0.9499, Dice: 0.9743\n",
      "Validation - Loss: 0.0529, IoU: 0.7803, Dice: 0.8737\n",
      "------------------------------------------------------------\n",
      "Epoch 21/50:\n",
      "Training - Loss: 0.0113, IoU: 0.9506, Dice: 0.9747\n",
      "Validation - Loss: 0.0543, IoU: 0.7775, Dice: 0.8721\n",
      "------------------------------------------------------------\n",
      "Epoch 22/50:\n",
      "Training - Loss: 0.0111, IoU: 0.9515, Dice: 0.9751\n",
      "Validation - Loss: 0.0550, IoU: 0.7779, Dice: 0.8723\n",
      "------------------------------------------------------------\n",
      "Epoch 23/50:\n",
      "Training - Loss: 0.0110, IoU: 0.9518, Dice: 0.9753\n",
      "Validation - Loss: 0.0546, IoU: 0.7746, Dice: 0.8703\n",
      "------------------------------------------------------------\n",
      "Epoch 24/50:\n",
      "Training - Loss: 0.0108, IoU: 0.9530, Dice: 0.9759\n",
      "Validation - Loss: 0.0570, IoU: 0.7740, Dice: 0.8699\n",
      "------------------------------------------------------------\n",
      "Epoch 25/50:\n",
      "Training - Loss: 0.0106, IoU: 0.9540, Dice: 0.9764\n",
      "Validation - Loss: 0.0559, IoU: 0.7762, Dice: 0.8712\n",
      "------------------------------------------------------------\n",
      "Epoch 26/50:\n",
      "Training - Loss: 0.0106, IoU: 0.9539, Dice: 0.9764\n",
      "Validation - Loss: 0.0567, IoU: 0.7756, Dice: 0.8709\n",
      "------------------------------------------------------------\n",
      "Epoch 27/50:\n",
      "Training - Loss: 0.0104, IoU: 0.9550, Dice: 0.9769\n",
      "Validation - Loss: 0.0576, IoU: 0.7744, Dice: 0.8701\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optimized_train_model(model, train_loader, test_loader, num_epochs=50, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可选保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"models/UnetTrain/overlaping_Ynet_segmentation_try4.27V0A1K.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试model性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2最新的predict全流程设置\n",
    "# 设置参数\n",
    "model_path = \"models/UnetTrain/overlaping_unet_segmentation_try4.13k_random2_iou.pth\"  # 模型路径\n",
    "\n",
    "image_path = \"dataset/Lucchi++/Test_In/mask0022.png\"  # 测试图片路径\n",
    "mask_path = \"dataset/Lucchi++/Test_Out/22.png\"  # 真实掩码路径（如果有）\n",
    "\n",
    "# image_path = \"dataset/Kasthuri++/Test_In/mask1049.png\"  \n",
    "# mask_path = \"dataset/Kasthuri++/Test_Out/mask1049.png\"  \n",
    "\n",
    "\n",
    "# image_path = \"dataset/VNC/Test_In/16.tif\"  \n",
    "# mask_path = \"dataset/VNC/Test_Out/16.png\"  \n",
    "\n",
    "save_dir = \"test/predictData\"  # 结果保存目录\n",
    "patch_size = 256  # patch大小\n",
    "stride = 128  # 步长\n",
    "\n",
    "# 执行分割流程\n",
    "pred_mask, metrics = segmentation_pipeline(\n",
    "    model_path=model_path,\n",
    "    image_path=image_path,\n",
    "    mask_path=mask_path,\n",
    "    save_dir=save_dir,\n",
    "    patch_size=patch_size,\n",
    "    stride=stride,\n",
    "    value=-30,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "if metrics:\n",
    "    print(f\"最终评估指标: IoU={metrics['IoU']:.4f}, Dice={metrics['Dice']:.4f}\")\n",
    "    \n",
    "plt.imshow(pred_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 部分image路径不存在\n",
      "Error: 无有效图像路径\n"
     ]
    }
   ],
   "source": [
    "# 处理特定图像列表\n",
    "image_list = [\"img1.png\", \"img2.png\", \"img3.png\"]\n",
    "mask_list = [\"mask1.png\", \"mask2.png\", \"mask3.png\"]\n",
    "\n",
    "results = batch_segmentation_pipeline(\n",
    "    model_path=\"models/UnetTrain/overlaping_unet_segmentation_try4.13k_random2_loss.pth\",\n",
    "    image_paths=image_list,\n",
    "    mask_paths=mask_list,\n",
    "    save_dir=\"test/batch_predictData\",\n",
    "    patch_size=256,\n",
    "    stride=128,\n",
    "    value=-30,\n",
    "    alpha=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批处理测试model效果，并保存结果同时生成对应的文件(可用，速度一般,1张图大概6s左右)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始处理 20 个样本...\n",
      "\n",
      "\n",
      " 处理进度 1/20: 00\n",
      "\n",
      "=== 处理图像: 00 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.6089, Dice=0.7569\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 48985\n",
      "denoise评估结果: IoU=0.5904, Dice=0.7425\n",
      "\n",
      "\n",
      " 处理进度 2/20: 01\n",
      "\n",
      "=== 处理图像: 01 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.5807, Dice=0.7347\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 44278\n",
      "denoise评估结果: IoU=0.5719, Dice=0.7277\n",
      "\n",
      "\n",
      " 处理进度 3/20: 02\n",
      "\n",
      "=== 处理图像: 02 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.5788, Dice=0.7332\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 37270\n",
      "denoise评估结果: IoU=0.5677, Dice=0.7242\n",
      "\n",
      "\n",
      " 处理进度 4/20: 03\n",
      "\n",
      "=== 处理图像: 03 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.5248, Dice=0.6883\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 33233\n",
      "denoise评估结果: IoU=0.5160, Dice=0.6808\n",
      "\n",
      "\n",
      " 处理进度 5/20: 04\n",
      "\n",
      "=== 处理图像: 04 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.5588, Dice=0.7170\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 32054\n",
      "denoise评估结果: IoU=0.5485, Dice=0.7085\n",
      "\n",
      "\n",
      " 处理进度 6/20: 05\n",
      "\n",
      "=== 处理图像: 05 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.5901, Dice=0.7422\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 34990\n",
      "denoise评估结果: IoU=0.5897, Dice=0.7419\n",
      "\n",
      "\n",
      " 处理进度 7/20: 06\n",
      "\n",
      "=== 处理图像: 06 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.4448, Dice=0.6157\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 27835\n",
      "denoise评估结果: IoU=0.4349, Dice=0.6061\n",
      "\n",
      "\n",
      " 处理进度 8/20: 07\n",
      "\n",
      "=== 处理图像: 07 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.4484, Dice=0.6192\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 32633\n",
      "denoise评估结果: IoU=0.4314, Dice=0.6028\n",
      "\n",
      "\n",
      " 处理进度 9/20: 08\n",
      "\n",
      "=== 处理图像: 08 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.4485, Dice=0.6193\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 31381\n",
      "denoise评估结果: IoU=0.4416, Dice=0.6127\n",
      "\n",
      "\n",
      " 处理进度 10/20: 09\n",
      "\n",
      "=== 处理图像: 09 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.5441, Dice=0.7048\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 37853\n",
      "denoise评估结果: IoU=0.5353, Dice=0.6973\n",
      "\n",
      "\n",
      " 处理进度 11/20: 10\n",
      "\n",
      "=== 处理图像: 10 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.4298, Dice=0.6012\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 33457\n",
      "denoise评估结果: IoU=0.4115, Dice=0.5830\n",
      "\n",
      "\n",
      " 处理进度 12/20: 11\n",
      "\n",
      "=== 处理图像: 11 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.3968, Dice=0.5682\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 30806\n",
      "denoise评估结果: IoU=0.3775, Dice=0.5481\n",
      "\n",
      "\n",
      " 处理进度 13/20: 12\n",
      "\n",
      "=== 处理图像: 12 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.3579, Dice=0.5271\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 28299\n",
      "denoise评估结果: IoU=0.3444, Dice=0.5123\n",
      "\n",
      "\n",
      " 处理进度 14/20: 13\n",
      "\n",
      "=== 处理图像: 13 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.3609, Dice=0.5303\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 27467\n",
      "denoise评估结果: IoU=0.3456, Dice=0.5137\n",
      "\n",
      "\n",
      " 处理进度 15/20: 14\n",
      "\n",
      "=== 处理图像: 14 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.3948, Dice=0.5661\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 27680\n",
      "denoise评估结果: IoU=0.3792, Dice=0.5499\n",
      "\n",
      "\n",
      " 处理进度 16/20: 15\n",
      "\n",
      "=== 处理图像: 15 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.4705, Dice=0.6399\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 37239\n",
      "denoise评估结果: IoU=0.4559, Dice=0.6263\n",
      "\n",
      "\n",
      " 处理进度 17/20: 16\n",
      "\n",
      "=== 处理图像: 16 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.4942, Dice=0.6615\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 38083\n",
      "denoise评估结果: IoU=0.4755, Dice=0.6445\n",
      "\n",
      "\n",
      " 处理进度 18/20: 17\n",
      "\n",
      "=== 处理图像: 17 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.4797, Dice=0.6484\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 38855\n",
      "denoise评估结果: IoU=0.4563, Dice=0.6267\n",
      "\n",
      "\n",
      " 处理进度 19/20: 18\n",
      "\n",
      "=== 处理图像: 18 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.5355, Dice=0.6975\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 46349\n",
      "denoise评估结果: IoU=0.5104, Dice=0.6758\n",
      "\n",
      "\n",
      " 处理进度 20/20: 19\n",
      "\n",
      "=== 处理图像: 19 ===\n",
      "设备: cuda, 模型: overlaping_unet_segmentation_try4.13k_random2_loss.pth\n",
      "Patch尺寸: 256, 步长: 128\n",
      "评估指标: IoU=0.6125, Dice=0.7597\n",
      "二值化前的图像范围: 0-255\n",
      "二值化阈值: 127\n",
      "二值化后白色像素数: 46915\n",
      "denoise评估结果: IoU=0.6077, Dice=0.7560\n",
      "\n",
      "----- 批量处理统计信息 -----\n",
      "平均 IoU: 0.4796 ± 0.0823\n",
      "平均 Dice: 0.6440 ± 0.0760\n",
      "最佳 IoU: 0.6077 (样本: 19)\n",
      "最差 IoU: 0.3444 (样本: 12)\n",
      "统计信息已保存至: test/batch_predictData\\iou_statistics\n"
     ]
    }
   ],
   "source": [
    "# 处理整个目录\n",
    "results = batch_segmentation_pipeline(\n",
    "    model_path=\"models/UnetTrain/overlaping_unet_segmentation_try4.13k_random2_loss.pth\",\n",
    "    image_paths=\"dataset/VNC/Test_In\",  # 图像目录\n",
    "    mask_paths=\"dataset/VNC/Test_Out\",  # 掩码目录\n",
    "    save_dir=\"test/batch_predictData\",\n",
    "    patch_size=256,\n",
    "    stride=128,\n",
    "    value=-30,\n",
    "    alpha=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.13遇到的问题：\n",
    " 首先是K测L的泛化很低，因为之前只是针对单张图片导致我错判以为IoU很高，实际上只有0.08\n",
    " 太低了，然后我尝试在分析情况发现一个问题，就是equalization带来的影响很大，甚至可以说直方图的均值后的分布直接决定了Iou，也就是说我equalization大概率写的不好，导致它均值的时候对不同图片效果不同，简单来说那几张很特殊IOU很高的图，它们的均值图都是很平均的一条线，而其它的图均值后基本跟原本的直方图分布差不多。。\n",
    " 解决： 实际上是因为文件mask的名称和image没有对上导致的意外，这个是很致命的，因为我pipeline的batch没做好，后面尝试修改就完成了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量重命名文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def rename_files_with_padded_numbers(folder_path, prefix=\"mask\"):\n",
    "    \"\"\"\n",
    "    将文件夹内的数字文件名重命名为带前缀和补零格式\n",
    "    \n",
    "    参数:\n",
    "        folder_path (str): 目标文件夹路径\n",
    "        prefix (str): 要添加的前缀（默认为\"mask\"）\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        old_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if os.path.isfile(old_path):\n",
    "            # 使用正则表达式提取数字部分\n",
    "            match = re.match(r'^(\\d+)\\.(.*?)$', filename)\n",
    "            if match:\n",
    "                number = match.group(1)\n",
    "                ext = match.group(2)\n",
    "                \n",
    "                # 将数字转为4位数（前面补零）\n",
    "                padded_number = f\"{int(number):04d}\"\n",
    "                \n",
    "                # 新文件名\n",
    "                new_filename = f\"{prefix}{padded_number}.{ext}\"\n",
    "                new_path = os.path.join(folder_path, new_filename)\n",
    "                \n",
    "                # 重命名文件\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"已重命名: {filename} -> {new_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \"dataset/Lucchi++/Test_Out\"\n",
    "    prefix = input(\"请输入要添加的前缀(默认为'mask'，直接回车使用默认值): \") or \"mask\"\n",
    "    \n",
    "    rename_files_with_padded_numbers(folder, prefix)\n",
    "    print(\"所有文件重命名完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试设定完善模型和训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义新的版本Unet框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class ImprovedUNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.2):\n",
    "        super(ImprovedUNet, self).__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.enc1 = self.conv_block(3, 32, dropout_rate)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc2 = self.conv_block(32, 64, dropout_rate)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc3 = self.conv_block(64, 128, dropout_rate)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc4 = self.conv_block(128, 256, dropout_rate)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(256, 512, dropout_rate)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec4 = self.conv_block(512, 256, dropout_rate)  # 512 = 256 + 256 (skip connection)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(256, 128, dropout_rate)  # 256 = 128 + 128 (skip connection)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(128, 64, dropout_rate)   # 128 = 64 + 64 (skip connection)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(64, 32, dropout_rate)    # 64 = 32 + 32 (skip connection)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.final = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels, dropout_rate):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "        enc3 = self.enc3(self.pool2(enc2))\n",
    "        enc4 = self.enc4(self.pool3(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.dec4(dec4)\n",
    "        \n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.dec3(dec3)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.final(dec1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def train_and_validate_model(model, train_loader, val_loader=None, num_epochs=50, \n",
    "                           learning_rate=1e-4, device='cuda', save_path='best_model.pth'):\n",
    "    \"\"\"\n",
    "    训练和验证模型 - 添加数据加载调试信息，优化内存管理\n",
    "    \n",
    "    参数:\n",
    "        model: 要训练的模型\n",
    "        train_loader: 训练数据加载器\n",
    "        val_loader: 验证数据加载器 (可选)\n",
    "        num_epochs: 训练轮数\n",
    "        learning_rate: 学习率\n",
    "        device: 训练设备 ('cuda' 或 'cpu')\n",
    "        save_path: 保存最佳模型的路径\n",
    "    \n",
    "    返回:\n",
    "        model: 训练后的模型\n",
    "        history: 训练历史记录\n",
    "    \"\"\"\n",
    "    # 移动模型到指定设备\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 设置优化器和损失函数\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # 创建学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # 记录训练历史\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_dice': [],\n",
    "        'val_dice': [],\n",
    "        'train_iou': [],\n",
    "        'val_iou': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    # 用于早停和保存最佳模型\n",
    "    best_val_metric = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    print(f\"开始训练，共 {num_epochs} 个轮次\")\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n开始第 {epoch+1}/{num_epochs} 轮训练\")\n",
    "        \n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice_scores = []\n",
    "        train_iou_scores = []\n",
    "        \n",
    "        # 清理GPU内存\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # 获取数据加载器的迭代器\n",
    "        train_iter = iter(train_loader)\n",
    "        \n",
    "        # 获取批次总数\n",
    "        total_batches = len(train_loader)\n",
    "        print(f\"此轮次共有 {total_batches} 个批次\")\n",
    "        \n",
    "        # 使用预先获取的迭代器，逐个批次处理\n",
    "        for batch_idx in range(total_batches):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # 尝试加载下一个批次\n",
    "            print(f\"正在加载批次 {batch_idx+1}/{total_batches}...\")\n",
    "            try:\n",
    "                batch = next(train_iter)\n",
    "                print(f\"批次 {batch_idx+1} 加载完成，耗时 {time.time() - batch_start_time:.2f}秒\")\n",
    "            except Exception as e:\n",
    "                print(f\"加载批次 {batch_idx+1} 时出错: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "            # 将数据转移到设备\n",
    "            data_to_device_time = time.time()\n",
    "            try:\n",
    "                # 逐个张量转移以避免整批内存问题\n",
    "                inputs = batch['patches'].to(device)\n",
    "                print(f\"输入数据转移到 {device} 完成，形状: {inputs.shape}\")\n",
    "                \n",
    "                targets = batch['mask_patches'].to(device)\n",
    "                print(f\"目标数据转移到 {device} 完成，形状: {targets.shape}\")\n",
    "                \n",
    "                print(f\"数据转移到 {device} 完成，耗时 {time.time() - data_to_device_time:.2f}秒\")\n",
    "            except Exception as e:\n",
    "                print(f\"将数据转移到 {device} 时出错: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "            # 梯度清零和前向传播\n",
    "            forward_time = time.time()\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                print(f\"前向传播完成，耗时 {time.time() - forward_time:.2f}秒\")\n",
    "            except Exception as e:\n",
    "                print(f\"前向传播时出错: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "            # 计算损失\n",
    "            loss_time = time.time()\n",
    "            try:\n",
    "                loss = criterion(outputs, targets)\n",
    "                print(f\"损失计算完成，耗时 {time.time() - loss_time:.2f}秒\")\n",
    "            except Exception as e:\n",
    "                print(f\"计算损失时出错: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "            # 反向传播和优化\n",
    "            backward_time = time.time()\n",
    "            try:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(f\"反向传播完成，耗时 {time.time() - backward_time:.2f}秒\")\n",
    "            except Exception as e:\n",
    "                print(f\"反向传播时出错: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "            # 累加损失\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # 计算训练指标\n",
    "            metrics_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                iou, dice = calculate_metrics(preds, targets)\n",
    "                train_dice_scores.append(dice)\n",
    "                train_iou_scores.append(iou)\n",
    "                print(f\"指标计算完成，耗时 {time.time() - metrics_time:.2f}秒\")\n",
    "            \n",
    "            # 手动清理临时张量以释放内存\n",
    "            del inputs, targets, outputs, preds, loss\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            batch_total_time = time.time() - batch_start_time\n",
    "            print(f\"批次 {batch_idx+1} 处理完成，总耗时 {batch_total_time:.2f}秒\")\n",
    "            \n",
    "            # 只处理第一个批次以进行诊断后退出\n",
    "            if batch_idx == 0 and epoch == 0:\n",
    "                print(\"\\n成功处理第一个批次！继续训练...\\n\")\n",
    "        \n",
    "        # 计算平均训练指标\n",
    "        train_loss /= total_batches\n",
    "        train_dice = sum(train_dice_scores) / len(train_dice_scores) if train_dice_scores else 0\n",
    "        train_iou = sum(train_iou_scores) / len(train_iou_scores) if train_iou_scores else 0\n",
    "        \n",
    "        # 记录当前学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # 保存训练指标\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['train_iou'].append(train_iou)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # 验证阶段\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_dice, val_iou = validate_model(model, val_loader, criterion, device)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_dice'].append(val_dice)\n",
    "            history['val_iou'].append(val_iou)\n",
    "            \n",
    "            # 调整学习率\n",
    "            old_lr = optimizer.param_groups[0]['lr']\n",
    "            scheduler.step(val_loss)\n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # 手动检测学习率变化并打印通知\n",
    "            if new_lr < old_lr:\n",
    "                print(f\"学习率从 {old_lr:.6f} 减小到 {new_lr:.6f}\")\n",
    "            \n",
    "            # 早停和保存最佳模型\n",
    "            if val_loss < best_val_metric:\n",
    "                best_val_metric = val_loss\n",
    "                counter = 0\n",
    "                # 保存最佳模型\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"轮次 {epoch+1}/{num_epochs} - \"\n",
    "                     f\"训练损失: {train_loss:.4f}, 训练Dice: {train_dice:.4f}, \"\n",
    "                     f\"验证损失: {val_loss:.4f}, 验证Dice: {val_dice:.4f} - \"\n",
    "                     f\"耗时: {time.time() - epoch_start_time:.1f}秒 - 保存最佳模型\")\n",
    "            else:\n",
    "                counter += 1\n",
    "                print(f\"轮次 {epoch+1}/{num_epochs} - \"\n",
    "                     f\"训练损失: {train_loss:.4f}, 训练Dice: {train_dice:.4f}, \"\n",
    "                     f\"验证损失: {val_loss:.4f}, 验证Dice: {val_dice:.4f} - \"\n",
    "                     f\"耗时: {time.time() - epoch_start_time:.1f}秒\")\n",
    "                \n",
    "                if counter >= patience:\n",
    "                    print(f\"早停: {epoch+1} 轮后未见改善\")\n",
    "                    break\n",
    "        else:\n",
    "            # 如果没有验证集，每个epoch都保存模型\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"轮次 {epoch+1}/{num_epochs} - \"\n",
    "                 f\"训练损失: {train_loss:.4f}, 训练Dice: {train_dice:.4f} - \"\n",
    "                 f\"耗时: {time.time() - epoch_start_time:.1f}秒\")\n",
    "    \n",
    "    # 如果有验证集，加载最佳模型\n",
    "    if val_loader is not None and os.path.exists(save_path):\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "    \n",
    "    return model, history    \n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    在验证集上评估模型 - 简化版，无进度显示\n",
    "    \n",
    "    参数:\n",
    "        model: 要评估的模型\n",
    "        val_loader: 验证数据加载器\n",
    "        criterion: 损失函数\n",
    "        device: 评估设备\n",
    "    \n",
    "    返回:\n",
    "        val_loss: 验证损失\n",
    "        val_dice: 验证Dice系数\n",
    "        val_iou: 验证IoU\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # 获取输入和目标\n",
    "            inputs = batch['patches'].to(device)\n",
    "            targets = batch['mask_patches'].to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # 计算评估指标\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            iou, dice = calculate_metrics(preds, targets)\n",
    "            dice_scores.append(dice)\n",
    "            iou_scores.append(iou)\n",
    "    \n",
    "    # 计算平均损失和评估指标\n",
    "    val_loss /= len(val_loader)\n",
    "    val_dice = sum(dice_scores) / len(dice_scores)\n",
    "    val_iou = sum(iou_scores) / len(iou_scores)\n",
    "    \n",
    "    return val_loss, val_dice, val_iou\n",
    "\n",
    "def calculate_metrics(pred_mask, true_mask, threshold=0.5):\n",
    "    \"\"\"\n",
    "    计算IoU和Dice系数\n",
    "    \n",
    "    参数:\n",
    "        pred_mask: 预测掩码（已经是二值化的掩码或概率掩码）\n",
    "        true_mask: 目标掩码\n",
    "        threshold: 二值化阈值（如果预测掩码还不是二值的）\n",
    "    \n",
    "    返回:\n",
    "        iou: IoU值\n",
    "        dice: Dice系数\n",
    "    \"\"\"\n",
    "    # 确保预测掩码是二值的\n",
    "    if threshold is not None:\n",
    "        pred_mask = (pred_mask > threshold).float()\n",
    "\n",
    "    # 计算交集和并集\n",
    "    intersection = (pred_mask * true_mask).sum()\n",
    "    union = pred_mask.sum() + true_mask.sum() - intersection\n",
    "\n",
    "    # 避免除零错误\n",
    "    smooth = 1e-7\n",
    "    \n",
    "    # 计算IoU\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "\n",
    "    # 计算Dice系数\n",
    "    dice = (2. * intersection + smooth) / (pred_mask.sum() + true_mask.sum() + smooth)\n",
    "\n",
    "    return iou.item(), dice.item()\n",
    "\n",
    "def reconstruct_from_patches(patches, positions, original_size, patch_size, stride):\n",
    "    \"\"\"\n",
    "    从patches重建完整图像\n",
    "    \n",
    "    参数:\n",
    "        patches: 预测的patch列表\n",
    "        positions: 每个patch的左上角坐标列表，格式为[(y1, x1), (y2, x2), ...]\n",
    "        original_size: 原始图像尺寸，格式为(height, width)\n",
    "        patch_size: patch的大小\n",
    "        stride: patch滑动的步长\n",
    "\n",
    "    返回:\n",
    "        reconstructed: 重建后的完整图像\n",
    "    \"\"\"\n",
    "    h, w = original_size\n",
    "    reconstructed = np.zeros((h, w), dtype=np.float32)\n",
    "    count = np.zeros((h, w), dtype=np.float32)\n",
    "    \n",
    "    for patch, (y, x) in zip(patches, positions):\n",
    "        patch_h = min(patch_size, h - y)\n",
    "        patch_w = min(patch_size, w - x)\n",
    "        reconstructed[y:y+patch_h, x:x+patch_w] += patch[:patch_h, :patch_w]\n",
    "        count[y:y+patch_h, x:x+patch_w] += 1\n",
    "    \n",
    "    # 处理重叠区域，确保没有除零错误\n",
    "    count[count == 0] = 1\n",
    "    reconstructed /= count\n",
    "    return reconstructed\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    可视化训练历史\n",
    "    \n",
    "    参数:\n",
    "        history: 包含训练和验证指标的字典\n",
    "        save_path: 可选，保存图像的路径\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    if 'val_loss' in history and history['val_loss']:\n",
    "        plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # IoU\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, history['train_iou'], 'b-', label='Training IoU')\n",
    "    if 'val_iou' in history and history['val_iou']:\n",
    "        plt.plot(epochs, history['val_iou'], 'r-', label='Validation IoU')\n",
    "    plt.title('Training and Validation IoU')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Dice\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, history['train_dice'], 'b-', label='Training Dice')\n",
    "    if 'val_dice' in history and history['val_dice']:\n",
    "        plt.plot(epochs, history['val_dice'], 'r-', label='Validation Dice')\n",
    "    plt.title('Training and Validation Dice')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Dice')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Learning Rate\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, history['lr'], 'g-', label='Learning Rate')\n",
    "    plt.title('Learning Rate')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Training history plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()    \n",
    "    \n",
    "  \n",
    "def improved_predict(model, image, mask=None, device=\"cuda\", patch_size=256, stride=None, \n",
    "                    test_time_augmentation=True, tta_flips=True, tta_scales=True):\n",
    "    \"\"\"\n",
    "    改进的预测函数：支持测试时增强\n",
    "    \n",
    "    参数:\n",
    "        model: 训练好的模型\n",
    "        image: 预处理后的图像数组（RGB格式），已确保尺寸适合patch提取\n",
    "        mask: 预处理后的真实掩码数组（可选，用于计算评估指标）\n",
    "        device: 使用的设备，默认为cuda\n",
    "        patch_size: 处理的patch大小\n",
    "        stride: patch滑动的步长，None则默认为patch_size//2\n",
    "        test_time_augmentation: 是否使用测试时增强\n",
    "        tta_flips: 是否使用翻转进行测试时增强\n",
    "        tta_scales: 是否使用多尺度进行测试时增强\n",
    "    \n",
    "    返回:\n",
    "        pred_mask: 预测的分割掩码\n",
    "        metrics: 评估指标（若提供真实掩码）\n",
    "    \"\"\"\n",
    "    # 设置默认stride\n",
    "    if stride is None:\n",
    "        stride = patch_size // 2\n",
    "    \n",
    "    # 获取图像尺寸\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # 执行预测\n",
    "    model.eval()\n",
    "    \n",
    "    # 计算步数\n",
    "    h_steps = max(1, (h - patch_size + stride) // stride)\n",
    "    w_steps = max(1, (w - patch_size + stride) // stride)\n",
    "    \n",
    "    patches_list = []\n",
    "    patch_positions = []\n",
    "    \n",
    "    # 提取重叠的patches\n",
    "    for i in range(h_steps):\n",
    "        for j in range(w_steps):\n",
    "            # 计算patch坐标\n",
    "            y_start = min(i * stride, h - patch_size)\n",
    "            x_start = min(j * stride, w - patch_size)\n",
    "            \n",
    "            # 提取patch\n",
    "            patch = image[y_start:y_start+patch_size, x_start:x_start+patch_size]\n",
    "            \n",
    "            # 处理可能的边界情况，确保patch尺寸正确\n",
    "            if patch.shape[0] < patch_size or patch.shape[1] < patch_size:\n",
    "                temp_patch = np.zeros((patch_size, patch_size, 3), dtype=patch.dtype)\n",
    "                temp_patch[:patch.shape[0], :patch.shape[1]] = patch\n",
    "                patch = temp_patch\n",
    "            \n",
    "            # 标准化和通道顺序转换\n",
    "            normalized_patch = patch.astype(np.float32) / 255.0\n",
    "            normalized_patch = normalized_patch.transpose(2, 0, 1)\n",
    "            patches_list.append(normalized_patch)\n",
    "            patch_positions.append((y_start, x_start))\n",
    "    \n",
    "    # 转换为tensor\n",
    "    patches_array = np.stack(patches_list)\n",
    "    patches_tensor = torch.from_numpy(patches_array).float().to(device)\n",
    "    \n",
    "    # 分批处理以避免内存不足\n",
    "    batch_size = 16  # 可以根据可用GPU内存调整\n",
    "    all_pred_patches = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(patches_tensor), batch_size):\n",
    "            batch = patches_tensor[i:i+batch_size]\n",
    "            \n",
    "            if test_time_augmentation:\n",
    "                # 测试时增强预测\n",
    "                pred_masks = predict_with_tta(\n",
    "                    model, batch, tta_flips=tta_flips, tta_scales=tta_scales\n",
    "                )\n",
    "            else:\n",
    "                # 常规预测\n",
    "                outputs = model(batch)\n",
    "                pred_masks = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            \n",
    "            all_pred_patches.extend([p[0].cpu().numpy() for p in pred_masks])\n",
    "    \n",
    "    # 重建完整的预测掩码\n",
    "    prediction = reconstruct_from_patches(\n",
    "        all_pred_patches, \n",
    "        patch_positions, \n",
    "        (h, w), \n",
    "        patch_size, \n",
    "        stride\n",
    "    )\n",
    "    \n",
    "    # 二值化\n",
    "    prediction = (prediction > 0.5).astype(np.float32)\n",
    "    \n",
    "    # 计算评估指标（如果提供了真实掩码）\n",
    "    metrics = None\n",
    "    if mask is not None:\n",
    "        # 确保掩码格式正确\n",
    "        # 假设255是前景，0和2是背景\n",
    "        mask_float = (mask == 255).astype(np.float32)\n",
    "            \n",
    "        # 转换为tensor计算IoU和Dice\n",
    "        pred_tensor = torch.from_numpy(prediction)\n",
    "        true_tensor = torch.from_numpy((mask_float > 0.5).astype(np.float32))\n",
    "        \n",
    "        # 确保尺寸一致\n",
    "        if pred_tensor.shape != true_tensor.shape:\n",
    "            print(f\"警告: 预测掩码 ({pred_tensor.shape}) 和真实掩码 ({true_tensor.shape}) 尺寸不一致\")\n",
    "            # 使用最近邻插值调整大小\n",
    "            pred_tensor = torch.nn.functional.interpolate(\n",
    "                pred_tensor.unsqueeze(0).unsqueeze(0), \n",
    "                size=true_tensor.shape, \n",
    "                mode='nearest'\n",
    "            ).squeeze(0).squeeze(0)\n",
    "        \n",
    "        # 计算评估指标\n",
    "        iou, dice = calculate_metrics(pred_tensor, true_tensor)\n",
    "        metrics = {\"IoU\": iou, \"Dice\": dice}\n",
    "        \n",
    "        print(f\"评估指标: IoU={iou:.4f}, Dice={dice:.4f}\")\n",
    "    \n",
    "    return prediction, metrics    \n",
    "    \n",
    "    \n",
    "    \n",
    "def predict_with_tta(model, x, tta_flips=True, tta_scales=True):\n",
    "    \"\"\"\n",
    "    使用测试时增强进行预测\n",
    "    \n",
    "    参数:\n",
    "        model: 模型\n",
    "        x: 输入张量\n",
    "        tta_flips: 是否使用翻转增强\n",
    "        tta_scales: 是否使用多尺度增强\n",
    "    \n",
    "    返回:\n",
    "        平均预测结果\n",
    "    \"\"\"\n",
    "    # 原始预测\n",
    "    original_pred = torch.sigmoid(model(x))\n",
    "    all_preds = [original_pred]\n",
    "    \n",
    "    # 水平翻转\n",
    "    if tta_flips:\n",
    "        # 水平翻转\n",
    "        flipped_h = torch.flip(x, dims=[3])  # 水平翻转\n",
    "        pred_h = torch.sigmoid(model(flipped_h))\n",
    "        pred_h = torch.flip(pred_h, dims=[3])  # 翻转回来\n",
    "        all_preds.append(pred_h)\n",
    "        \n",
    "        # 垂直翻转\n",
    "        flipped_v = torch.flip(x, dims=[2])  # 垂直翻转\n",
    "        pred_v = torch.sigmoid(model(flipped_v))\n",
    "        pred_v = torch.flip(pred_v, dims=[2])  # 翻转回来\n",
    "        all_preds.append(pred_v)\n",
    "        \n",
    "        # 水平+垂直翻转\n",
    "        flipped_hv = torch.flip(x, dims=[2, 3])  # 水平+垂直翻转\n",
    "        pred_hv = torch.sigmoid(model(flipped_hv))\n",
    "        pred_hv = torch.flip(pred_hv, dims=[2, 3])  # 翻转回来\n",
    "        all_preds.append(pred_hv)\n",
    "    \n",
    "    # 多尺度\n",
    "    if tta_scales:\n",
    "        # 缩小\n",
    "        scale_small = F.interpolate(x, scale_factor=0.75, mode='bilinear', align_corners=False)\n",
    "        pred_small = torch.sigmoid(model(scale_small))\n",
    "        pred_small = F.interpolate(pred_small, size=original_pred.shape[2:], mode='bilinear', align_corners=False)\n",
    "        all_preds.append(pred_small)\n",
    "        \n",
    "        # 放大\n",
    "        scale_large = F.interpolate(x, scale_factor=1.25, mode='bilinear', align_corners=False)\n",
    "        pred_large = torch.sigmoid(model(scale_large))\n",
    "        pred_large = F.interpolate(pred_large, size=original_pred.shape[2:], mode='bilinear', align_corners=False)\n",
    "        all_preds.append(pred_large)\n",
    "    \n",
    "    # 平均所有预测\n",
    "    final_pred = torch.stack(all_preds).mean(dim=0)\n",
    "    return (final_pred > 0.5).float()\n",
    "\n",
    "\n",
    "def post_process_mask(mask, min_size=100, close_kernel_size=5, open_kernel_size=3):\n",
    "    \"\"\"\n",
    "    对预测掩码进行后处理以提高质量\n",
    "    \n",
    "    参数:\n",
    "        mask: 预测的二值掩码\n",
    "        min_size: 移除小于此大小的连通区域\n",
    "        close_kernel_size: 闭运算的核大小\n",
    "        open_kernel_size: 开运算的核大小\n",
    "    \n",
    "    返回:\n",
    "        后处理后的掩码\n",
    "    \"\"\"\n",
    "    # 确保掩码是二值的\n",
    "    binary_mask = (mask > 0.5).astype(np.uint8)\n",
    "    \n",
    "    # 闭运算（先膨胀后腐蚀）填充小洞\n",
    "    if close_kernel_size > 0:\n",
    "        close_kernel = np.ones((close_kernel_size, close_kernel_size), np.uint8)\n",
    "        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, close_kernel)\n",
    "    \n",
    "    # 开运算（先腐蚀后膨胀）移除小噪点\n",
    "    if open_kernel_size > 0:\n",
    "        open_kernel = np.ones((open_kernel_size, open_kernel_size), np.uint8)\n",
    "        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, open_kernel)\n",
    "    \n",
    "    # 移除小连通区域\n",
    "    if min_size > 0:\n",
    "        # 标记连通区域\n",
    "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary_mask, connectivity=8)\n",
    "        \n",
    "        # 移除小于min_size的区域\n",
    "        for i in range(1, num_labels):  # 从1开始，跳过背景\n",
    "            if stats[i, cv2.CC_STAT_AREA] < min_size:\n",
    "                binary_mask[labels == i] = 0\n",
    "    \n",
    "    return binary_mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义新的dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedSegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list, patch_size=128, stride=64, is_training=True, preload=True, transform=None):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        Args:\n",
    "            data_list: 包含图像和标注路径的列表\n",
    "            patch_size: 切片大小\n",
    "            stride: 滑动窗口步长\n",
    "            is_training: 是否为训练模式，决定是否应用随机预处理\n",
    "            preload: 是否预加载所有patch到内存\n",
    "            transform: 数据增强转换\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.is_training = is_training\n",
    "        self.preload = preload\n",
    "        self.transform = transform\n",
    "        self.preprocessor = ImagePreprocessor()\n",
    "        \n",
    "        # 存储图像索引和可能的patch位置\n",
    "        self.patches_info = []\n",
    "        \n",
    "        # 预加载数据的存储\n",
    "        if preload:\n",
    "            self.all_patches = []\n",
    "            self.all_masks = []\n",
    "            self.all_positions = []\n",
    "            self.all_original_sizes = []\n",
    "            self.all_image_paths = []\n",
    "            self.all_mask_paths = []\n",
    "            self.image_indices = []\n",
    "        \n",
    "        print(f\"{'训练' if is_training else '验证/测试'}数据集初始化...\")\n",
    "        \n",
    "        # 处理每张图片\n",
    "        for idx, item in enumerate(tqdm(data_list, desc=\"处理图像\")):\n",
    "            # 读取图像以获取尺寸\n",
    "            image = cv2.imread(item[\"image\"])\n",
    "            if image is None:\n",
    "                print(f\"警告: 无法读取图像 {item['image']}\")\n",
    "                continue\n",
    "                \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            mask = cv2.imread(item[\"annotation\"], cv2.IMREAD_GRAYSCALE)\n",
    "            if mask is None:\n",
    "                print(f\"警告: 无法读取掩码 {item['annotation']}\")\n",
    "                continue\n",
    "            \n",
    "            # 如果预加载，立即预处理并保存所有patch\n",
    "            if preload:\n",
    "                # 预处理图像\n",
    "                if is_training:\n",
    "                    processed_image, processed_mask, params = self.preprocessor.random_preprocess(\n",
    "                        image, mask, patch_size=self.patch_size\n",
    "                    )\n",
    "                else:\n",
    "                    processed_image, processed_mask = self.preprocessor.preprocess(\n",
    "                        image, mask, patch_size=self.patch_size\n",
    "                    )\n",
    "                \n",
    "                # 计算可能的patch位置\n",
    "                h, w = processed_image.shape[:2]\n",
    "                positions = self._calculate_positions(h, w)\n",
    "                \n",
    "                # 提取和保存所有patch\n",
    "                for y, x in positions:\n",
    "                    # 确保坐标不超出预处理后图像的边界\n",
    "                    y_valid = min(y, h - self.patch_size)\n",
    "                    x_valid = min(x, w - self.patch_size)\n",
    "                    \n",
    "                    # 提取patch\n",
    "                    patch = processed_image[y_valid:y_valid+self.patch_size, x_valid:x_valid+self.patch_size].copy()\n",
    "                    mask_patch = processed_mask[y_valid:y_valid+self.patch_size, x_valid:x_valid+self.patch_size].copy()\n",
    "                    \n",
    "                    # 应用变换（如果有）\n",
    "                    if self.transform:\n",
    "                        # 假设transform是一个可调用对象，接受图像和掩码\n",
    "                        augmented = self.transform(image=patch, mask=mask_patch)\n",
    "                        patch = augmented['image']\n",
    "                        mask_patch = augmented['mask']\n",
    "                    \n",
    "                    # 转换为tensor\n",
    "                    patch_tensor = torch.FloatTensor(patch.transpose(2, 0, 1)) / 255.0\n",
    "                    mask_tensor = torch.FloatTensor(mask_patch).unsqueeze(0) / 255.0\n",
    "                    \n",
    "                    # 存储结果\n",
    "                    self.all_patches.append(patch_tensor)\n",
    "                    self.all_masks.append(mask_tensor)\n",
    "                    self.all_positions.append((y_valid, x_valid))\n",
    "                    self.all_original_sizes.append((h, w))\n",
    "                    self.all_image_paths.append(item[\"image\"])\n",
    "                    self.all_mask_paths.append(item[\"annotation\"])\n",
    "                    self.image_indices.append(idx)\n",
    "            else:\n",
    "                # 如果不预加载，只保存图像索引和位置信息\n",
    "                h, w = image.shape[:2]\n",
    "                positions = self._calculate_positions(h, w)\n",
    "                \n",
    "                # 存储图像索引和patch位置\n",
    "                for pos in positions:\n",
    "                    self.patches_info.append({\n",
    "                        'image_idx': idx,\n",
    "                        'position': pos\n",
    "                    })\n",
    "        \n",
    "        if preload:\n",
    "            print(f\"数据集预处理完成，共生成 {len(self.all_patches)} 个patch，来自 {len(data_list)} 张图像\")\n",
    "        else:\n",
    "            print(f\"数据集索引完成，共索引 {len(self.patches_info)} 个可能的patch，来自 {len(data_list)} 张图像\")\n",
    "        \n",
    "        # 图像缓存，用于非预加载模式\n",
    "        self._cached_image_idx = None\n",
    "        self._cached_image = None\n",
    "        self._cached_mask = None\n",
    "\n",
    "    def _calculate_positions(self, h, w):\n",
    "        \"\"\"计算图像中所有有效的patch位置\"\"\"\n",
    "        positions = []\n",
    "        \n",
    "        # 横向和纵向的滑动位置\n",
    "        h_idx = np.arange(0, h-self.patch_size+1, self.stride)\n",
    "        w_idx = np.arange(0, w-self.patch_size+1, self.stride)\n",
    "        \n",
    "        # 确保处理到边缘\n",
    "        if h % self.stride != 0 and h > self.patch_size:\n",
    "            h_idx = np.append(h_idx, h-self.patch_size)\n",
    "        if w % self.stride != 0 and w > self.patch_size:\n",
    "            w_idx = np.append(w_idx, w-self.patch_size)\n",
    "        \n",
    "        # 生成所有位置组合\n",
    "        for y in h_idx:\n",
    "            for x in w_idx:\n",
    "                positions.append((int(y), int(x)))\n",
    "                \n",
    "        return positions\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.preload:\n",
    "            return len(self.all_patches)\n",
    "        else:\n",
    "            return len(self.patches_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload:\n",
    "            # 预加载模式直接返回保存的数据\n",
    "            return {\n",
    "                'patches': self.all_patches[idx],\n",
    "                'mask_patches': self.all_masks[idx],\n",
    "                'positions': self.all_positions[idx],\n",
    "                'original_size': self.all_original_sizes[idx],\n",
    "                'image_path': self.all_image_paths[idx],\n",
    "                'mask_path': self.all_mask_paths[idx],\n",
    "                'image_idx': self.image_indices[idx]\n",
    "            }\n",
    "        else:\n",
    "            # 动态加载模式\n",
    "            # 获取patch信息\n",
    "            patch_info = self.patches_info[idx]\n",
    "            image_idx = patch_info['image_idx']\n",
    "            position = patch_info['position']\n",
    "            \n",
    "            # 使用图像缓存减少I/O\n",
    "            if self._cached_image_idx != image_idx:\n",
    "                # 获取原图和掩码路径\n",
    "                image_path = self.data_list[image_idx][\"image\"]\n",
    "                mask_path = self.data_list[image_idx][\"annotation\"]\n",
    "                \n",
    "                # 读取图像和掩码\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                # 使用预处理方法\n",
    "                if self.is_training:\n",
    "                    processed_image, processed_mask, params = self.preprocessor.random_preprocess(\n",
    "                        image, mask, patch_size=self.patch_size\n",
    "                    )\n",
    "                else:\n",
    "                    processed_image, processed_mask = self.preprocessor.preprocess(\n",
    "                        image, mask, patch_size=self.patch_size\n",
    "                    )\n",
    "                \n",
    "                # 更新缓存\n",
    "                self._cached_image_idx = image_idx\n",
    "                self._cached_image = processed_image\n",
    "                self._cached_mask = processed_mask\n",
    "                self._cached_image_path = image_path\n",
    "                self._cached_mask_path = mask_path\n",
    "            else:\n",
    "                # 使用缓存\n",
    "                processed_image = self._cached_image\n",
    "                processed_mask = self._cached_mask\n",
    "                image_path = self._cached_image_path\n",
    "                mask_path = self._cached_mask_path\n",
    "            \n",
    "            # 提取patch\n",
    "            y, x = position\n",
    "            \n",
    "            # 确保坐标不超出预处理后图像的边界\n",
    "            h, w = processed_image.shape[:2]\n",
    "            y = min(y, h - self.patch_size)\n",
    "            x = min(x, w - self.patch_size)\n",
    "            \n",
    "            # 复制避免视图问题\n",
    "            patch = processed_image[y:y+self.patch_size, x:x+self.patch_size].copy()\n",
    "            mask_patch = processed_mask[y:y+self.patch_size, x:x+self.patch_size].copy()\n",
    "            \n",
    "            # 应用变换（如果有）\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=patch, mask=mask_patch)\n",
    "                patch = augmented['image']\n",
    "                mask_patch = augmented['mask']\n",
    "            \n",
    "            # 转换为tensor\n",
    "            patch = torch.FloatTensor(patch.transpose(2, 0, 1)) / 255.0\n",
    "            mask_patch = torch.FloatTensor(mask_patch).unsqueeze(0) / 255.0\n",
    "            \n",
    "            return {\n",
    "                'patches': patch,\n",
    "                'mask_patches': mask_patch,\n",
    "                'positions': (y, x),\n",
    "                'original_size': (h, w),\n",
    "                'image_path': image_path,\n",
    "                'mask_path': mask_path,\n",
    "                'image_idx': image_idx\n",
    "            }\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"返回数据集的统计信息\"\"\"\n",
    "        if self.preload:\n",
    "            unique_images = len(set(self.image_indices))\n",
    "            patches_per_image = len(self.all_patches) / unique_images if unique_images > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'total_patches': len(self.all_patches),\n",
    "                'unique_images': unique_images,\n",
    "                'patches_per_image': patches_per_image,\n",
    "                'patch_size': self.patch_size,\n",
    "                'stride': self.stride,\n",
    "                'is_training': self.is_training,\n",
    "                'preload': self.preload\n",
    "            }\n",
    "        else:\n",
    "            unique_images = len(set(info['image_idx'] for info in self.patches_info))\n",
    "            patches_per_image = len(self.patches_info) / unique_images if unique_images > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'total_patches': len(self.patches_info),\n",
    "                'unique_images': unique_images,\n",
    "                'patches_per_image': patches_per_image,\n",
    "                'patch_size': self.patch_size,\n",
    "                'stride': self.stride,\n",
    "                'is_training': self.is_training,\n",
    "                'preload': self.preload\n",
    "            }\n",
    "        \n",
    "    def visualize_batch(self, indices=None, num_samples=5):\n",
    "        \"\"\"\n",
    "        可视化一批数据\n",
    "        \n",
    "        Args:\n",
    "            indices: 要可视化的样本索引列表，如果为None则随机选择\n",
    "            num_samples: 如果indices为None，要随机选择的样本数量\n",
    "        \"\"\"\n",
    "        if indices is None:\n",
    "            indices = np.random.choice(len(self), size=min(num_samples, len(self)), replace=False)\n",
    "        \n",
    "        for idx in indices:\n",
    "            sample = self[idx]\n",
    "            \n",
    "            # 转换数据格式用于显示\n",
    "            patch = sample['patches'].numpy()\n",
    "            mask = sample['mask_patches'].numpy()\n",
    "            \n",
    "            patch = np.transpose(patch, (1, 2, 0))  # CHW -> HWC\n",
    "            mask = np.squeeze(mask)\n",
    "            \n",
    "            # 还原归一化\n",
    "            patch = (patch * 255).astype(np.uint8)\n",
    "            mask = (mask * 255).astype(np.uint8)\n",
    "            \n",
    "            # 创建子图\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "            image_name = os.path.basename(sample[\"image_path\"])\n",
    "            fig.suptitle(f'Sample #{idx} from {image_name}')\n",
    "            \n",
    "            # 显示原始patch\n",
    "            axes[0].imshow(patch)\n",
    "            axes[0].set_title('Image Patch')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # 显示二值mask\n",
    "            axes[1].imshow(mask, cmap='gray')\n",
    "            axes[1].set_title('Mask')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            # 显示叠加效果\n",
    "            overlay = patch.copy()\n",
    "            overlay[mask > 127] = [255, 0, 0]  # 用红色标注细胞区域\n",
    "            axes[2].imshow(overlay)\n",
    "            axes[2].set_title('Overlay')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def visualize_image_patches(self, image_path, num_patches=5):\n",
    "        \"\"\"可视化指定图片的patches分布\"\"\"\n",
    "        try:\n",
    "            # 找到指定图片在数据集中的索引位置\n",
    "            image_indices = [i for i, item in enumerate(self.data_list) if item[\"image\"] == image_path]\n",
    "            \n",
    "            if not image_indices:\n",
    "                print(f\"未找到图片: {image_path}\")\n",
    "                return\n",
    "                \n",
    "            image_idx = image_indices[0]\n",
    "            \n",
    "            # 获取对应的mask路径\n",
    "            mask_path = self.data_list[image_idx][\"annotation\"]\n",
    "            \n",
    "            # 读取原始图片和对应的mask\n",
    "            original_image = cv2.imread(image_path)\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            original_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # 进行预处理（与__getitem__中相同）\n",
    "            if self.is_training:\n",
    "                processed_image, processed_mask, _ = self.preprocessor.random_preprocess(\n",
    "                    original_image, original_mask, patch_size=self.patch_size\n",
    "                )\n",
    "            else:\n",
    "                processed_image, processed_mask = self.preprocessor.preprocess(\n",
    "                    original_image, original_mask, patch_size=self.patch_size\n",
    "                )\n",
    "            \n",
    "            # 找到这个图像的所有patch索引\n",
    "            patch_indices = [i for i, info in enumerate(self.patches_info) \n",
    "                            if info['image_idx'] == image_idx]\n",
    "            \n",
    "            if not patch_indices:\n",
    "                print(f\"未找到图片的patches: {image_path}\")\n",
    "                return\n",
    "            \n",
    "            # 随机选择要显示的patches\n",
    "            if len(patch_indices) <= num_patches:\n",
    "                selected_indices = patch_indices\n",
    "            else:\n",
    "                selected_indices = random.sample(patch_indices, num_patches)\n",
    "                \n",
    "            num_selected = len(selected_indices)\n",
    "            \n",
    "            # 创建图像网格\n",
    "            plt.figure(figsize=(20, 8))\n",
    "            \n",
    "            # 显示处理后的图像和patches的位置\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(processed_image)\n",
    "            plt.title(\"Processed Image with Patch Locations\")\n",
    "            \n",
    "            # 显示处理后的mask和patches的位置\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(processed_mask, cmap='gray')\n",
    "            plt.title(\"Processed Mask with Patch Locations\")\n",
    "            \n",
    "            # 在图像和mask上标注patch位置\n",
    "            colors = plt.cm.rainbow(np.linspace(0, 1, num_selected))\n",
    "            \n",
    "            for subplot_idx in [1, 2]:\n",
    "                plt.subplot(1, 2, subplot_idx)\n",
    "                \n",
    "                for i, idx in enumerate(selected_indices):\n",
    "                    patch_info = self.patches_info[idx]\n",
    "                    y, x = patch_info['position']\n",
    "                    \n",
    "                    # 确保坐标不超出边界\n",
    "                    h, w = processed_image.shape[:2]\n",
    "                    y = min(y, h - self.patch_size)\n",
    "                    x = min(x, w - self.patch_size)\n",
    "                    \n",
    "                    rect = plt.Rectangle(\n",
    "                        xy=(x, y),\n",
    "                        width=self.patch_size,\n",
    "                        height=self.patch_size,\n",
    "                        fill=False,\n",
    "                        color=colors[i],\n",
    "                        linewidth=2\n",
    "                    )\n",
    "                    plt.gca().add_patch(rect)\n",
    "                    \n",
    "                    plt.text(\n",
    "                        x, y,\n",
    "                        str(i+1), \n",
    "                        color=colors[i], \n",
    "                        fontsize=12, \n",
    "                        bbox=dict(facecolor='white', alpha=0.7)\n",
    "                    )\n",
    "                plt.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 显示每个patch的详细信息\n",
    "            for i, idx in enumerate(selected_indices):\n",
    "                sample = self[idx]\n",
    "                \n",
    "                # 转换数据格式\n",
    "                patch = sample['patches'].numpy()\n",
    "                mask = sample['mask_patches'].numpy()\n",
    "                \n",
    "                patch = np.transpose(patch, (1, 2, 0))\n",
    "                mask = np.squeeze(mask)\n",
    "                \n",
    "                # 还原归一化\n",
    "                patch = (patch * 255).astype(np.uint8)\n",
    "                mask = (mask * 255).astype(np.uint8)\n",
    "                \n",
    "                # 创建子图\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "                fig.suptitle(f'Patch {i+1}')\n",
    "                \n",
    "                # 显示原始patch\n",
    "                axes[0].imshow(patch)\n",
    "                axes[0].set_title('Image Patch')\n",
    "                axes[0].axis('off')\n",
    "                \n",
    "                # 显示二值mask\n",
    "                axes[1].imshow(mask, cmap='gray')\n",
    "                axes[1].set_title('Mask')\n",
    "                axes[1].axis('off')\n",
    "                \n",
    "                # 显示叠加效果\n",
    "                overlay = patch.copy()\n",
    "                overlay[mask > 127] = [255, 0, 0]\n",
    "                axes[2].imshow(overlay)\n",
    "                axes[2].set_title('Overlay')\n",
    "                axes[2].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"可视化过程出错: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "\n",
    "def get_optimized_loaders(train_dataset, val_dataset, batch_size=32, num_workers=4, prefetch_factor=2):\n",
    "    \"\"\"\n",
    "    创建优化的数据加载器\n",
    "    \n",
    "    参数:\n",
    "        train_dataset: 训练数据集对象\n",
    "        val_dataset: 验证数据集对象\n",
    "        batch_size: 批量大小\n",
    "        num_workers: 数据加载工作线程数\n",
    "        prefetch_factor: 预取因子\n",
    "        \n",
    "    返回:\n",
    "        train_loader: 训练数据加载器\n",
    "        val_loader: 验证数据加载器\n",
    "    \"\"\"\n",
    "    # 自动检测CPU核心数并设置工作线程\n",
    "    if num_workers <= 0:\n",
    "        import os\n",
    "        num_workers = min(os.cpu_count(), 8)\n",
    "        print(f\"自动设置工作线程数: {num_workers}\")\n",
    "    \n",
    "    # 设置共享内存管理策略，避免共享内存问题\n",
    "    import torch\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    \n",
    "    # 创建训练数据加载器\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,  # 训练时随机打乱数据\n",
    "        num_workers=num_workers,  # 并行加载数据\n",
    "        pin_memory=True,  # 使用固定内存加速GPU传输\n",
    "        prefetch_factor=prefetch_factor,  # 预取因子\n",
    "        persistent_workers=True if num_workers > 0 else False,  # 保持工作线程存活\n",
    "        drop_last=True  # 丢弃不完整的最后一个batch，避免批归一化问题\n",
    "    )\n",
    "    \n",
    "    # 创建验证数据加载器\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,  # 验证时不打乱顺序\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        drop_last=False  # 验证时保留所有样本\n",
    "    )\n",
    "    \n",
    "    # 打印加载器信息\n",
    "    print(f\"数据加载器创建完成:\")\n",
    "    print(f\"- 训练集: {len(train_dataset)} 个样本, {len(train_loader)} 个批次\")\n",
    "    print(f\"- 验证集: {len(val_dataset)} 个样本, {len(val_loader)} 个批次\")\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = prepare_dataset(\"Kasthuri++\")\n",
    "# train_data, val_data = prepare_dataset(\"Lucchi++\")\n",
    "\n",
    "# 第一次运行时：预处理并保存\n",
    "train_dataset = ImprovedSegmentationDataset(\n",
    "    data_list=train_data,\n",
    "    patch_size=256,\n",
    "    stride=128,\n",
    "    preProcess=True,  # 启用预处理\n",
    "\n",
    ")\n",
    "\n",
    "val_dataset = ImprovedSegmentationDataset(\n",
    "    data_list=val_data,\n",
    "    patch_size=256,\n",
    "    stride=128,\n",
    "    preProcess=True,  # 启用预处理\n",
    "\n",
    ")\n",
    "\n",
    "train_loader, val_loader = get_optimized_loaders(\n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    batch_size=32,  # 增加批大小\n",
    "    num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = ImprovedUNet(num_classes=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"随机种子已设置为 {seed}\")\n",
    "\n",
    "# 设置随机种子\n",
    "set_seed(42)\n",
    "\n",
    "# 检查并创建保存模型和结果的目录\n",
    "results_dir = \"results\"\n",
    "models_dir = os.path.join(results_dir, \"models\")\n",
    "plots_dir = os.path.join(results_dir, \"plots\")\n",
    "\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# 选择数据集\n",
    "dataset_name = \"Kasthuri++\"  # 或 \"Lucchi++\"\n",
    "print(f\"使用数据集: {dataset_name}\")\n",
    "\n",
    "# 准备数据\n",
    "train_data, val_data = prepare_dataset(dataset_name)\n",
    "print(f\"训练数据: {len(train_data)} 张图像\")\n",
    "print(f\"验证数据: {len(val_data)} 张图像\")\n",
    "\n",
    "# 创建优化的数据集，使用预加载模式\n",
    "train_dataset = OptimizedSegmentationDataset(\n",
    "    data_list=train_data,\n",
    "    patch_size=256,\n",
    "    stride=128,\n",
    "    is_training=True,\n",
    "    preload=True  # 预加载所有数据到内存以提高速度\n",
    ")\n",
    "\n",
    "val_dataset = OptimizedSegmentationDataset(\n",
    "    data_list=val_data,\n",
    "    patch_size=256,\n",
    "    stride=128,\n",
    "    is_training=True,\n",
    "    preload=True  # 预加载所有数据到内存以提高速度\n",
    ")\n",
    "\n",
    "# 打印数据集统计信息\n",
    "train_stats = train_dataset.get_stats()\n",
    "val_stats = val_dataset.get_stats()\n",
    "print(f\"训练数据集统计: {train_stats}\")\n",
    "print(f\"验证数据集统计: {val_stats}\")\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader, val_loader = get_optimized_loaders(\n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 初始化模型\n",
    "model = ImprovedUNet(num_classes=1, dropout_rate=0.2)\n",
    "model = model.to(device)\n",
    "print(f\"模型已创建并移至 {device}\")\n",
    "\n",
    "# 打印模型总结\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"模型参数: 总计 {total_params/1e6:.2f}M, 可训练 {trainable_params/1e6:.2f}M\")\n",
    "\n",
    "# 设置训练参数\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "model_save_path = os.path.join(models_dir, f\"{dataset_name}_unet_model.pth\")\n",
    "\n",
    "# 修改后的训练函数使用非tqdm进度条\n",
    "def train_and_validate_model(model, train_loader, val_loader=None, num_epochs=50, \n",
    "                           learning_rate=1e-4, device='cuda', save_path='best_model.pth'):\n",
    "    \"\"\"\n",
    "    训练和验证模型 - 简化无进度条版本\n",
    "    \"\"\"\n",
    "    # 移动模型到指定设备\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 设置优化器和损失函数\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # 创建学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # 记录训练历史\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_dice': [],\n",
    "        'val_dice': [],\n",
    "        'train_iou': [],\n",
    "        'val_iou': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    # 用于早停和保存最佳模型\n",
    "    best_val_metric = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    print(f\"开始训练，共 {num_epochs} 个轮次\")\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_dice_scores = []\n",
    "        train_iou_scores = []\n",
    "        \n",
    "        total_batches = len(train_loader)\n",
    "        batch_count = 0\n",
    "        \n",
    "        # 手动处理批次\n",
    "        for batch in train_loader:\n",
    "            # 获取输入和目标\n",
    "            inputs = batch['patches'].to(device)\n",
    "            targets = batch['mask_patches'].to(device)\n",
    "            \n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 累加损失\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # 计算训练指标\n",
    "            with torch.no_grad():\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                iou, dice = calculate_metrics(preds, targets)\n",
    "                train_dice_scores.append(dice)\n",
    "                train_iou_scores.append(iou)\n",
    "            \n",
    "            batch_count += 1\n",
    "            \n",
    "            # 显示部分进度\n",
    "            if batch_count % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - 批次 {batch_count}/{total_batches}\")\n",
    "        \n",
    "        # 计算平均训练指标\n",
    "        train_loss /= total_batches\n",
    "        train_dice = sum(train_dice_scores) / len(train_dice_scores)\n",
    "        train_iou = sum(train_iou_scores) / len(train_iou_scores)\n",
    "        \n",
    "        # 记录当前学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # 保存训练指标\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['train_iou'].append(train_iou)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # 验证阶段\n",
    "        if val_loader is not None:\n",
    "            val_loss, val_dice, val_iou = validate_model(model, val_loader, criterion, device)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_dice'].append(val_dice)\n",
    "            history['val_iou'].append(val_iou)\n",
    "            \n",
    "            # 调整学习率\n",
    "            old_lr = optimizer.param_groups[0]['lr']\n",
    "            scheduler.step(val_loss)\n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # 手动检测学习率变化并打印通知\n",
    "            if new_lr < old_lr:\n",
    "                print(f\"学习率从 {old_lr:.6f} 减小到 {new_lr:.6f}\")\n",
    "            \n",
    "            # 早停和保存最佳模型\n",
    "            if val_loss < best_val_metric:\n",
    "                best_val_metric = val_loss\n",
    "                counter = 0\n",
    "                # 保存最佳模型\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"轮次 {epoch+1}/{num_epochs} - \"\n",
    "                     f\"训练损失: {train_loss:.4f}, 训练Dice: {train_dice:.4f}, \"\n",
    "                     f\"验证损失: {val_loss:.4f}, 验证Dice: {val_dice:.4f} - \"\n",
    "                     f\"耗时: {time.time() - epoch_start_time:.1f}秒 - 保存最佳模型\")\n",
    "            else:\n",
    "                counter += 1\n",
    "                print(f\"轮次 {epoch+1}/{num_epochs} - \"\n",
    "                     f\"训练损失: {train_loss:.4f}, 训练Dice: {train_dice:.4f}, \"\n",
    "                     f\"验证损失: {val_loss:.4f}, 验证Dice: {val_dice:.4f} - \"\n",
    "                     f\"耗时: {time.time() - epoch_start_time:.1f}秒\")\n",
    "                \n",
    "                if counter >= patience:\n",
    "                    print(f\"早停: {epoch+1} 轮后未见改善\")\n",
    "                    break\n",
    "        else:\n",
    "            # 如果没有验证集，每个epoch都保存模型\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"轮次 {epoch+1}/{num_epochs} - \"\n",
    "                 f\"训练损失: {train_loss:.4f}, 训练Dice: {train_dice:.4f} - \"\n",
    "                 f\"耗时: {time.time() - epoch_start_time:.1f}秒\")\n",
    "    \n",
    "    # 如果有验证集，加载最佳模型\n",
    "    if val_loader is not None and os.path.exists(save_path):\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# 简化的验证函数\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"在验证集上评估模型 - 简化无进度条版本\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # 获取输入和目标\n",
    "            inputs = batch['patches'].to(device)\n",
    "            targets = batch['mask_patches'].to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # 计算评估指标\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            iou, dice = calculate_metrics(preds, targets)\n",
    "            dice_scores.append(dice)\n",
    "            iou_scores.append(iou)\n",
    "    \n",
    "    # 计算平均损失和评估指标\n",
    "    val_loss /= len(val_loader)\n",
    "    val_dice = sum(dice_scores) / len(dice_scores)\n",
    "    val_iou = sum(iou_scores) / len(iou_scores)\n",
    "    \n",
    "    return val_loss, val_dice, val_iou\n",
    "\n",
    "# 训练模型\n",
    "print(f\"开始训练模型，总计 {num_epochs} 轮...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model, history = train_and_validate_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device,\n",
    "    save_path=model_save_path\n",
    ")\n",
    "\n",
    "# 计算总训练时间\n",
    "total_time = time.time() - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "print(f\"训练完成，总耗时: {hours}小时 {minutes}分钟 {seconds}秒\")\n",
    "\n",
    "# 绘制并保存训练历史\n",
    "history_plot_path = os.path.join(plots_dir, f\"{dataset_name}_training_history.png\")\n",
    "plot_training_history(history, save_path=history_plot_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
