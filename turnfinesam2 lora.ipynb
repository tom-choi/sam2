{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0000.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\0.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0001.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\1.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0002.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\2.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0003.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\3.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0004.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\4.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0005.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\5.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0006.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\6.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0007.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\7.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0008.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\8.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0009.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\9.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0010.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\10.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0011.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\11.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0012.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\12.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0013.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\13.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0014.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\14.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0015.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\15.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0016.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\16.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0017.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\17.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0018.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\18.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0019.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\19.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0020.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\20.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0021.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\21.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0022.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\22.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0023.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\23.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0024.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\24.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0025.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\25.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0026.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\26.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0027.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\27.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0028.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\28.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0029.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\29.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0030.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\30.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0031.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\31.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0032.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\32.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0033.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\33.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0034.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\34.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0035.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\35.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0036.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\36.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0037.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\37.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0038.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\38.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0039.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\39.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0040.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\40.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0041.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\41.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0042.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\42.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0043.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\43.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0044.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\44.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0045.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\45.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0046.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\46.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0047.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\47.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0048.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\48.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0049.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\49.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0050.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\50.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0051.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\51.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0052.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\52.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0053.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\53.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0054.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\54.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0055.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\55.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0056.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\56.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0057.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\57.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0058.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\58.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0059.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\59.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0060.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\60.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0061.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\61.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0062.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\62.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0063.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\63.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0064.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\64.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0065.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\65.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0066.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\66.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0067.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\67.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0068.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\68.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0069.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\69.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0070.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\70.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0071.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\71.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0072.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\72.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0073.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\73.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0074.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\74.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0075.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\75.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0076.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\76.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0077.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\77.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0078.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\78.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0079.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\79.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0080.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\80.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0081.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\81.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0082.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\82.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0083.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\83.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0084.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\84.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0085.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\85.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0086.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\86.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0087.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\87.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0088.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\88.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0089.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\89.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0090.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\90.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0091.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\91.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0092.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\92.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0093.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\93.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0094.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\94.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0095.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\95.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0096.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\96.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0097.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\97.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0098.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\98.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0099.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\99.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0100.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\100.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0101.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\101.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0102.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\102.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0103.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\103.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0104.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\104.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0105.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\105.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0106.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\106.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0107.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\107.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0108.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\108.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0109.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\109.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0110.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\110.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0111.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\111.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0112.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\112.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0113.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\113.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0114.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\114.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0115.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\115.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0116.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\116.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0117.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\117.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0118.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\118.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0119.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\119.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0120.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\120.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0121.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\121.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0122.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\122.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0123.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\123.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0124.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\124.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0125.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\125.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0126.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\126.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0127.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\127.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0128.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\128.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0129.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\129.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0130.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\130.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0131.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\131.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0132.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\132.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0133.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\133.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0134.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\134.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0135.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\135.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0136.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\136.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0137.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\137.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0138.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\138.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0139.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\139.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0140.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\140.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0141.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\141.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0142.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\142.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0143.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\143.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0144.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\144.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0145.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\145.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0146.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\146.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0147.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\147.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0148.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\148.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0149.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\149.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0150.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\150.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0151.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\151.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0152.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\152.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0153.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\153.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0154.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\154.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0155.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\155.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0156.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\156.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0157.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\157.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0158.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\158.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0159.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\159.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0160.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\160.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0161.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\161.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0162.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\162.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0163.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\163.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0164.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\164.png'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ... [前面的數據加載和預處理代碼保持不變] ...\n",
    "\n",
    "# Path to the chest-ct-segmentation dataset folder\n",
    "data_dir = \"dataset\\Lucchi++\"\n",
    "train_images_dir = os.path.join(data_dir, \"Train_In\")\n",
    "train_masks_dir = os.path.join(data_dir, \"Train_Out\")\n",
    "test_images_dir = os.path.join(data_dir, \"Test_In\")\n",
    "test_masks_dir = os.path.join(data_dir, \"Test_Out\")\n",
    "\n",
    "i = 0\n",
    "# Prepare the training data, Append image and corresponding mask paths\n",
    "train_data = []\n",
    "for image_file in os.listdir(train_images_dir):\n",
    "    image_path = os.path.join(train_images_dir, image_file)\n",
    "    mask_path = os.path.join(train_masks_dir, f\"{i}.png\")\n",
    "    i += 1\n",
    "    train_data.append(\n",
    "    { \n",
    "        \"image\" : image_path, \n",
    "        \"annotation\" : mask_path\n",
    "    })\n",
    "\n",
    "i = 0\n",
    "# Prepare the test data, Append image and corresponding mask paths\n",
    "test_data = []\n",
    "for image_file in os.listdir(test_images_dir):\n",
    "    image_path = os.path.join(test_images_dir, image_file)\n",
    "    mask_path = os.path.join(test_masks_dir, f\"{i}.png\")\n",
    "    i += 1\n",
    "    test_data.append(\n",
    "    { \n",
    "        \"image\" : image_path, \n",
    "        \"annotation\" : mask_path\n",
    "    })\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from skimage import measure\n",
    "\n",
    "def read_batch(data, visualize_data=False):\n",
    "    # 選擇隨機條目\n",
    "    ent = data[np.random.randint(len(data))]\n",
    "\n",
    "    # 讀取圖像\n",
    "    Img = cv2.imread(ent[\"image\"])[..., ::-1]  # 轉換BGR為RGB\n",
    "    ann_map = cv2.imread(ent[\"annotation\"], cv2.IMREAD_GRAYSCALE)  # 以灰度圖讀取註釋\n",
    "\n",
    "    if Img is None or ann_map is None:\n",
    "        print(f\"錯誤：無法從路徑 {ent['image']} 或 {ent['annotation']} 讀取圖像或遮罩\")\n",
    "        return None, None, None, 0\n",
    "\n",
    "    # 調整圖像和遮罩大小\n",
    "    r = min(1024 / Img.shape[1], 1024 / Img.shape[0])  # 縮放因子\n",
    "    Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "    ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 初始化二值遮罩\n",
    "    binary_mask = np.zeros_like(ann_map, dtype=np.uint8)\n",
    "\n",
    "    # 獲取二值遮罩並合併為單一遮罩\n",
    "    inds = np.unique(ann_map)[1:]  # 跳過背景（索引0）\n",
    "    for ind in inds:\n",
    "        mask = (ann_map == ind).astype(np.uint8)  # 為每個唯一索引創建二值遮罩\n",
    "        binary_mask = np.maximum(binary_mask, mask)  # 與現有二值遮罩合併\n",
    "\n",
    "    # 腐蝕合併的二值遮罩以避免邊界點\n",
    "    eroded_mask = cv2.erode(binary_mask, np.ones((5, 5), np.uint8), iterations=1)\n",
    "\n",
    "    # 使用連通區域分析來找到所有獨立的白色區域\n",
    "    labels = measure.label(eroded_mask)\n",
    "    regions = measure.regionprops(labels)\n",
    "\n",
    "    points = []\n",
    "    for region in regions:\n",
    "        # 為每個區域選擇一個隨機點\n",
    "        y, x = region.coords[np.random.randint(len(region.coords))]\n",
    "        points.append([x, y])  # 注意：我們存儲為 [x, y] 以與原始代碼保持一致\n",
    "\n",
    "    points = np.array(points)\n",
    "\n",
    "    if visualize_data:\n",
    "        # Plotting the images and points\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Original Image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(Img)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Segmentation Mask (binary_mask)\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Binarized Mask')\n",
    "        plt.imshow(binary_mask, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Mask with Points in Different Colors\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Binarized Mask with Points')\n",
    "        plt.imshow(binary_mask, cmap='gray')\n",
    "\n",
    "        # Plot points in different colors\n",
    "        colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "        for i, point in enumerate(points):\n",
    "            plt.scatter(point[0], point[1], c=colors[i % len(colors)], s=100, label=f'Point {i+1}')  # Corrected to plot y, x order\n",
    "\n",
    "        # plt.legend()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    binary_mask = np.expand_dims(binary_mask, axis=-1)  # 現在形狀是 (1024, 1024, 1)\n",
    "    binary_mask = binary_mask.transpose((2, 0, 1))\n",
    "    points = np.expand_dims(points, axis=1)\n",
    "\n",
    "    # 返回圖像、二值化遮罩、點和遮罩數量\n",
    "    return Img, binary_mask, points, len(inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, rank=4, scaling=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "\n",
    "        # 保存原始層\n",
    "        self.linear = linear_layer\n",
    "\n",
    "        # LoRA 組件\n",
    "        self.lora_down = nn.Linear(self.in_features, rank, bias=False)\n",
    "        self.lora_up = nn.Linear(rank, self.out_features, bias=False)\n",
    "        self.scaling = scaling\n",
    "\n",
    "        # 初始化\n",
    "        nn.init.kaiming_uniform_(self.lora_down.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "\n",
    "        # 凍結原始權重\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 原始層的輸出\n",
    "        orig_output = self.linear(x)\n",
    "        # LoRA 路徑\n",
    "        lora_output = self.lora_up(self.lora_down(x)) * self.scaling\n",
    "        return orig_output + lora_output\n",
    "\n",
    "def add_lora_to_model(model, rank=4, scaling=1.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    將 LoRA 添加到模型的關鍵組件，並確保所有組件都在正確的設備上\n",
    "    \"\"\"\n",
    "    modified_layers = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and any(key in name for key in [\n",
    "            'sam_prompt_encoder', 'sam_mask_decoder'\n",
    "        ]):\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            child_name = name.split('.')[-1]\n",
    "            parent_module = model\n",
    "\n",
    "            for part in parent_name.split('.'):\n",
    "                if part:\n",
    "                    parent_module = getattr(parent_module, part)\n",
    "\n",
    "            original_layer = getattr(parent_module, child_name)\n",
    "            # 創建 LoRA 層並移到指定設備\n",
    "            lora_layer = LoRALinear(original_layer, rank=rank, scaling=scaling).to(device)\n",
    "            setattr(parent_module, child_name, lora_layer)\n",
    "            modified_layers.append((name, lora_layer))\n",
    "\n",
    "    return modified_layers\n",
    "\n",
    "def train_with_lora(predictor, train_data, num_steps=3000, device=\"cuda\"):\n",
    "    # 確保模型在正確的設備上\n",
    "    predictor.model = predictor.model.to(device)\n",
    "\n",
    "    # 添加 LoRA 層\n",
    "    modified_layers = add_lora_to_model(predictor.model, rank=4, scaling=1.0, device=device)\n",
    "\n",
    "    if not modified_layers:\n",
    "        raise ValueError(\"No layers were modified with LoRA!\")\n",
    "\n",
    "    # 收集需要訓練的參數\n",
    "    trainable_params = []\n",
    "    for _, layer in modified_layers:\n",
    "        trainable_params.extend([\n",
    "            layer.lora_down.weight,\n",
    "            layer.lora_up.weight\n",
    "        ])\n",
    "\n",
    "    if not trainable_params:\n",
    "        raise ValueError(\"No trainable parameters found!\")\n",
    "\n",
    "    # 配置優化器\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        trainable_params,\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.2)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    accumulation_steps = 4\n",
    "\n",
    "    print(f\"Training {len(trainable_params)} LoRA parameters\")\n",
    "    mean_iou = 0\n",
    "\n",
    "    for step in range(1, num_steps + 1):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            image, mask, input_point, num_masks = read_batch(train_data, visualize_data=False)\n",
    "            if image is None or mask is None or num_masks == 0:\n",
    "                continue\n",
    "\n",
    "            input_label = np.ones((num_masks, 1))\n",
    "            if not isinstance(input_point, np.ndarray) or not isinstance(input_label, np.ndarray):\n",
    "                continue\n",
    "\n",
    "            if input_point.size == 0 or input_label.size == 0:\n",
    "                continue\n",
    "\n",
    "            # 將數據移到 GPU\n",
    "            predictor.set_image(image)\n",
    "            mask_input, ucc, labels, unnorm_box = predictor._prep_prompts(\n",
    "                input_point, input_label, box=None, mask_logits=None, normalize_coords=True\n",
    "            )\n",
    "\n",
    "            if ucc is None or labels is None or ucc.shape[0] == 0 or labels.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            for i in range(ucc.shape[0]):\n",
    "                uc = ucc[i:i+1, :, :]\n",
    "                # 確保輸入在正確的設備上\n",
    "                uc = uc.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "                    points=(uc, labels), boxes=None, masks=None,\n",
    "                )\n",
    "\n",
    "                batched_mode = uc.shape[0] > 1\n",
    "                high_res_features = [feat_level[-1].unsqueeze(0).to(device) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "\n",
    "                low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "                    image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0).to(device),\n",
    "                    image_pe=predictor.model.sam_prompt_encoder.get_dense_pe().to(device),\n",
    "                    sparse_prompt_embeddings=sparse_embeddings,\n",
    "                    dense_prompt_embeddings=dense_embeddings,\n",
    "                    multimask_output=True,\n",
    "                    repeat_image=batched_mode,\n",
    "                    high_res_features=high_res_features,\n",
    "                )\n",
    "\n",
    "                prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "                gt_mask = torch.tensor(mask.astype(np.float32), device=device)\n",
    "                prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "\n",
    "                seg_loss = (-gt_mask * torch.log(prd_mask + 1e-6) -\n",
    "                          (1 - gt_mask) * torch.log(1 - prd_mask + 1e-6)).mean()\n",
    "\n",
    "                inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "                union = gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter\n",
    "                iou = inter / union\n",
    "                score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "\n",
    "                loss = seg_loss + score_loss * 0.05\n",
    "                loss = loss / accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "            if step % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if step % 500 == 0:\n",
    "                lora_state = {}\n",
    "                for name, layer in modified_layers:\n",
    "                    lora_state[f\"{name}.lora_down.weight\"] = layer.lora_down.weight\n",
    "                    lora_state[f\"{name}.lora_up.weight\"] = layer.lora_up.weight\n",
    "                torch.save(lora_state, f\"sam2_lora_checkpoint_{step}.pth\")\n",
    "\n",
    "            mean_iou = mean_iou * 0.99 + 0.01 * torch.mean(iou).item()\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Step {step}:\\tAccuracy (IoU) = {mean_iou:.4f}\")\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 100 LoRA parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\github\\segment-anything-2\\sam2\\modeling\\backbones\\hieradet.py:70: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "f:\\github\\segment-anything-2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Memory efficient kernel not used because: (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:607.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "f:\\github\\segment-anything-2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at ..\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:495.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "f:\\github\\segment-anything-2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Flash attention kernel not used because: (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:609.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "f:\\github\\segment-anything-2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: CuDNN attention kernel not used because: (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:611.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "f:\\github\\segment-anything-2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: The CuDNN backend needs to be enabled by setting the enviornment variable`TORCH_CUDNN_SDPA_ENABLED=1` (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:410.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "f:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541: UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
      "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam2_checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "predictor = SAM2ImagePredictor(build_sam2(model_cfg, sam2_checkpoint, device=device))\n",
    "\n",
    "predictor = train_with_lora(predictor, train_data, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path, mask_path):  # read and resize image and mask\n",
    "   img = cv2.imread(image_path)[..., ::-1]  # Convert BGR to RGB\n",
    "   mask = cv2.imread(mask_path, 0)\n",
    "   r = np.min([1024 / img.shape[1], 1024 / img.shape[0]])\n",
    "   img = cv2.resize(img, (int(img.shape[1] * r), int(img.shape[0] * r)))\n",
    "   mask = cv2.resize(mask, (int(mask.shape[1] * r), int(mask.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n",
    "   return img, mask\n",
    "\n",
    "def get_points(mask, num_points):  # Sample points inside the input mask\n",
    "   points = []\n",
    "   coords = np.argwhere(mask > 0)\n",
    "   for i in range(num_points):\n",
    "       yx = np.array(coords[np.random.randint(len(coords))])\n",
    "       points.append([[yx[1], yx[0]]])\n",
    "   return np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_auto_prompts(image, strategy=\"combined\"):\n",
    "    \"\"\"\n",
    "    自动为图像生成提示点的函数\n",
    "\n",
    "    Args:\n",
    "        image: 输入图像\n",
    "        strategy: 提示点生成策略 [\"threshold\", \"gradient\", \"combined\"]\n",
    "\n",
    "    Returns:\n",
    "        points: numpy array of shape (N, 2) 包含提示点坐标\n",
    "    \"\"\"\n",
    "    # 转换为灰度图\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = image.copy()\n",
    "\n",
    "    points = []\n",
    "\n",
    "    if strategy == \"threshold\" or strategy == \"combined\":\n",
    "        # 1. 基于阈值的方法\n",
    "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for contour in contours:\n",
    "            # 计算轮廓的中心点\n",
    "            M = cv2.moments(contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                points.append([cx, cy])\n",
    "\n",
    "    if strategy == \"gradient\" or strategy == \"combined\":\n",
    "        # 2. 基于梯度的方法\n",
    "        # Sobel 边缘检测\n",
    "        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "\n",
    "        # 选择梯度最强的点\n",
    "        threshold = np.percentile(gradient_magnitude, 95)  # 选择前5%的强梯度点\n",
    "        strong_gradient = gradient_magnitude > threshold\n",
    "        gradient_points = np.where(strong_gradient)\n",
    "\n",
    "        # 从强梯度点中随机选择一些点\n",
    "        if len(gradient_points[0]) > 0:\n",
    "            indices = np.random.choice(len(gradient_points[0]),\n",
    "                                     min(5, len(gradient_points[0])),\n",
    "                                     replace=False)\n",
    "            for idx in indices:\n",
    "                points.append([gradient_points[1][idx], gradient_points[0][idx]])\n",
    "\n",
    "    return np.array(points)\n",
    "\n",
    "def sam2_inference_pipeline(image, sam_model):\n",
    "    \"\"\"\n",
    "    SAM2推理流水线\n",
    "\n",
    "    Args:\n",
    "        image: 输入图像\n",
    "        sam_model: 加载好的SAM2模型\n",
    "\n",
    "    Returns:\n",
    "        masks: 分割结果\n",
    "    \"\"\"\n",
    "    # 1. 自动生成提示点\n",
    "    prompt_points = generate_auto_prompts(image)\n",
    "\n",
    "    # 2. 对每个提示点进行预测\n",
    "    masks = []\n",
    "    labels = np.ones(len(prompt_points))  # 假设所有点都是前景\n",
    "\n",
    "    # 3. SAM2推理\n",
    "    for i in range(0, len(prompt_points), 5):  # 每批处理5个点\n",
    "        batch_points = prompt_points[i:i+5]\n",
    "        batch_labels = labels[i:i+5]\n",
    "\n",
    "        # SAM2模型预测\n",
    "        masks_batch = sam_model.predict(\n",
    "            point_coords=batch_points,\n",
    "            point_labels=batch_labels,\n",
    "        )\n",
    "        masks.extend(masks_batch)\n",
    "\n",
    "    # 4. 后处理\n",
    "    final_mask = post_process_masks(masks)\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "def post_process_masks(masks):\n",
    "    \"\"\"\n",
    "    后处理函数，合并和优化多个掩码\n",
    "    \"\"\"\n",
    "    if not masks:\n",
    "        return None\n",
    "\n",
    "    # 将所有掩码合并为一个\n",
    "    combined_mask = np.zeros_like(masks[0])\n",
    "    for mask in masks:\n",
    "        combined_mask = np.logical_or(combined_mask, mask)\n",
    "\n",
    "    # 形态学操作清理掩码\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    cleaned_mask = cv2.morphologyEx(combined_mask.astype(np.uint8),\n",
    "                                  cv2.MORPH_OPEN,\n",
    "                                  kernel)\n",
    "    cleaned_mask = cv2.morphologyEx(cleaned_mask,\n",
    "                                  cv2.MORPH_CLOSE,\n",
    "                                  kernel)\n",
    "\n",
    "    return cleaned_mask\n",
    "\n",
    "# 示例使用\n",
    "def process_medical_image(image_path, mask_path, sam_model):\n",
    "    \"\"\"\n",
    "    处理医学图像的完整流程\n",
    "    \"\"\"\n",
    "    # 读取图像\n",
    "    image, mask = read_image(image_path, mask_path)\n",
    "\n",
    "    # 自动生成提示点并进行SAM2推理\n",
    "    final_mask = sam2_inference_pipeline(image, sam_model)\n",
    "\n",
    "    # 可视化结果\n",
    "    if final_mask is not None:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(141)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original Image')\n",
    "\n",
    "        plt.subplot(142)\n",
    "        plt.imshow(mask)\n",
    "        plt.title('Ground Truth')\n",
    "\n",
    "        plt.subplot(143)\n",
    "        plt.imshow(final_mask, cmap='gray')\n",
    "        plt.title('Segmentation Mask')\n",
    "\n",
    "        plt.subplot(144)\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(final_mask, alpha=0.4, cmap='jet')\n",
    "        plt.title('Overlay')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m image_path \u001b[38;5;241m=\u001b[39m selected_entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m selected_entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m process_medical_image(image_path\u001b[38;5;241m=\u001b[39mimage_path, mask_path\u001b[38;5;241m=\u001b[39mmask_path, predictor\u001b[38;5;241m=\u001b[39m\u001b[43mpredictor\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
