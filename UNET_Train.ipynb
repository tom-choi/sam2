{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0000.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\0.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0001.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\1.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0002.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\2.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0003.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\3.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0004.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\4.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0005.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\5.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0006.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\6.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0007.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\7.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0008.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\8.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0009.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\9.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0010.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\10.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0011.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\11.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0012.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\12.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0013.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\13.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0014.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\14.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0015.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\15.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0016.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\16.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0017.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\17.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0018.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\18.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0019.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\19.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0020.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\20.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0021.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\21.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0022.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\22.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0023.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\23.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0024.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\24.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0025.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\25.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0026.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\26.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0027.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\27.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0028.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\28.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0029.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\29.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0030.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\30.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0031.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\31.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0032.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\32.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0033.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\33.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0034.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\34.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0035.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\35.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0036.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\36.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0037.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\37.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0038.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\38.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0039.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\39.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0040.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\40.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0041.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\41.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0042.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\42.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0043.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\43.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0044.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\44.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0045.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\45.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0046.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\46.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0047.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\47.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0048.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\48.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0049.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\49.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0050.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\50.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0051.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\51.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0052.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\52.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0053.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\53.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0054.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\54.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0055.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\55.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0056.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\56.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0057.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\57.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0058.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\58.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0059.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\59.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0060.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\60.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0061.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\61.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0062.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\62.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0063.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\63.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0064.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\64.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0065.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\65.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0066.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\66.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0067.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\67.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0068.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\68.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0069.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\69.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0070.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\70.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0071.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\71.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0072.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\72.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0073.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\73.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0074.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\74.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0075.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\75.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0076.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\76.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0077.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\77.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0078.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\78.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0079.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\79.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0080.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\80.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0081.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\81.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0082.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\82.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0083.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\83.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0084.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\84.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0085.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\85.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0086.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\86.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0087.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\87.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0088.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\88.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0089.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\89.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0090.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\90.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0091.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\91.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0092.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\92.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0093.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\93.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0094.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\94.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0095.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\95.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0096.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\96.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0097.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\97.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0098.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\98.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0099.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\99.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0100.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\100.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0101.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\101.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0102.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\102.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0103.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\103.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0104.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\104.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0105.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\105.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0106.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\106.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0107.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\107.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0108.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\108.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0109.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\109.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0110.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\110.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0111.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\111.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0112.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\112.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0113.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\113.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0114.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\114.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0115.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\115.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0116.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\116.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0117.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\117.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0118.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\118.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0119.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\119.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0120.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\120.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0121.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\121.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0122.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\122.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0123.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\123.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0124.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\124.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0125.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\125.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0126.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\126.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0127.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\127.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0128.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\128.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0129.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\129.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0130.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\130.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0131.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\131.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0132.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\132.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0133.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\133.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0134.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\134.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0135.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\135.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0136.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\136.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0137.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\137.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0138.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\138.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0139.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\139.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0140.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\140.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0141.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\141.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0142.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\142.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0143.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\143.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0144.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\144.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0145.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\145.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0146.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\146.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0147.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\147.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0148.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\148.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0149.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\149.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0150.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\150.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0151.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\151.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0152.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\152.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0153.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\153.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0154.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\154.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0155.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\155.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0156.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\156.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0157.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\157.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0158.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\158.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0159.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\159.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0160.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\160.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0161.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\161.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0162.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\162.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0163.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\163.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0164.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\164.png'}]\n"
     ]
    }
   ],
   "source": [
    "# Path to the chest-ct-segmentation dataset folder\n",
    "data_dir = \"dataset\\Lucchi++\"\n",
    "train_images_dir = os.path.join(data_dir, \"Train_In\")\n",
    "train_masks_dir = os.path.join(data_dir, \"Train_Out\")\n",
    "test_images_dir = os.path.join(data_dir, \"Test_In\")\n",
    "test_masks_dir = os.path.join(data_dir, \"Test_Out\")\n",
    "PATCH_SIZE = 128\n",
    "\n",
    "i = 0\n",
    "# Prepare the training data, Append image and corresponding mask paths\n",
    "train_data = []\n",
    "for image_file in os.listdir(train_images_dir):\n",
    "    image_path = os.path.join(train_images_dir, image_file)\n",
    "    mask_path = os.path.join(train_masks_dir, f\"{i}.png\")\n",
    "    i += 1\n",
    "    train_data.append(\n",
    "    { \n",
    "        \"image\" : image_path, \n",
    "        \"annotation\" : mask_path\n",
    "    })\n",
    "\n",
    "i = 0\n",
    "# Prepare the test data, Append image and corresponding mask paths\n",
    "test_data = []\n",
    "for image_file in os.listdir(test_images_dir):\n",
    "    image_path = os.path.join(test_images_dir, image_file)\n",
    "    mask_path = os.path.join(test_masks_dir, f\"{i}.png\")\n",
    "    i += 1\n",
    "    test_data.append(\n",
    "    { \n",
    "        \"image\" : image_path, \n",
    "        \"annotation\" : mask_path\n",
    "    })\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.conv_block(3, 32, stride=2),\n",
    "            self.conv_block(32, 64, stride=2),\n",
    "            self.conv_block(64, 128, stride=2),\n",
    "            self.conv_block(128, 256, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upconv_block(256, 128),\n",
    "            self.upconv_block(128, 64),\n",
    "            self.upconv_block(64, 32),\n",
    "            self.upconv_block(32, 32)\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.final = nn.Conv2d(32, num_classes, kernel_size=3, padding=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels, stride=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = []\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        # Decoder\n",
    "        for i, decoder_layer in enumerate(self.decoder):\n",
    "            x = decoder_layer(x)\n",
    "            if i < len(self.decoder) - 1:\n",
    "                x = x + features[-i-2]  # Skip connection\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = cv2.imread(self.data_list[idx][\"image\"])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.data_list[idx][\"annotation\"], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize image and mask to 1024x768\n",
    "        image = cv2.resize(image, (1024, 768))\n",
    "        mask = cv2.resize(mask, (1024, 768))\n",
    "\n",
    "        # Split image and mask into 64x64 patches\n",
    "        patch_size = PATCH_SIZE\n",
    "        image_patches = []\n",
    "        mask_patches = []\n",
    "\n",
    "        for i in range(0, 1024, patch_size):\n",
    "            for j in range(0, 768, patch_size):\n",
    "                image_patch = image[i:i+patch_size, j:j+patch_size]\n",
    "                mask_patch = mask[i:i+patch_size, j:j+patch_size]\n",
    "                if image_patch.size > 0 and mask_patch.size > 0:\n",
    "                    # Transpose image patch to (C, H, W) format\n",
    "                    image_patch = image_patch.transpose(2, 0, 1)\n",
    "                    image_patches.append(image_patch)\n",
    "                    mask_patches.append(mask_patch)\n",
    "\n",
    "        # Convert lists to numpy arrays and normalize\n",
    "        image_patches = np.array(image_patches, dtype=np.float32) / 255.0\n",
    "        mask_patches = np.array(mask_patches, dtype=np.float32) / 255.0\n",
    "\n",
    "        # Convert to tensors\n",
    "        image_patches = torch.from_numpy(image_patches)\n",
    "        mask_patches = torch.from_numpy(mask_patches).unsqueeze(1)\n",
    "\n",
    "        return image_patches, mask_patches\n",
    "\n",
    "def calculate_metrics(pred_mask, true_mask, threshold=0.5):\n",
    "    # Convert predictions to binary\n",
    "    pred_mask = (pred_mask > threshold).float()\n",
    "\n",
    "    # Calculate intersection and union\n",
    "    intersection = (pred_mask * true_mask).sum()\n",
    "    union = pred_mask.sum() + true_mask.sum() - intersection\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = (intersection + 1e-7) / (union + 1e-7)\n",
    "\n",
    "    # Calculate Dice coefficient\n",
    "    dice = (2. * intersection + 1e-7) / (pred_mask.sum() + true_mask.sum() + 1e-7)\n",
    "\n",
    "    return iou.item(), dice.item()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, device=\"cuda\"):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_iou = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_dice = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            images = images.view(-1, 3, PATCH_SIZE, PATCH_SIZE)\n",
    "            masks = masks.view(-1, 1, PATCH_SIZE, PATCH_SIZE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate metrics\n",
    "            batch_iou, batch_dice = calculate_metrics(torch.sigmoid(outputs), masks)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_iou += batch_iou\n",
    "            train_dice += batch_dice\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_train_loss = train_loss / num_batches\n",
    "        avg_train_iou = train_iou / num_batches\n",
    "        avg_train_dice = train_dice / num_batches\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "        val_dice = 0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                images = images.view(-1, 3, PATCH_SIZE, PATCH_SIZE)\n",
    "                masks = masks.view(-1, 1, PATCH_SIZE, PATCH_SIZE)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "                # Calculate metrics\n",
    "                batch_iou, batch_dice = calculate_metrics(torch.sigmoid(outputs), masks)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_iou += batch_iou\n",
    "                val_dice += batch_dice\n",
    "                num_val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / num_val_batches\n",
    "        avg_val_iou = val_iou / num_val_batches\n",
    "        avg_val_dice = val_dice / num_val_batches\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training - Loss: {avg_train_loss:.4f}, IoU: {avg_train_iou:.4f}, Dice: {avg_train_dice:.4f}')\n",
    "        print(f'Validation - Loss: {avg_val_loss:.4f}, IoU: {avg_val_iou:.4f}, Dice: {avg_val_dice:.4f}')\n",
    "\n",
    "        # Save best model based on IoU\n",
    "        if avg_val_iou > best_val_iou:\n",
    "            best_val_iou = avg_val_iou\n",
    "            torch.save(model.state_dict(), 'best_model_iou.pth')\n",
    "\n",
    "        # Also save based on loss if needed\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_loss.pth')\n",
    "\n",
    "        print('-' * 60)\n",
    "\n",
    "# Inference function\n",
    "def predict(model, image_path, device=\"cuda\"):\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (1024, 768))\n",
    "\n",
    "    # Split image into 64x64 patches\n",
    "    patch_size = PATCH_SIZE\n",
    "    patches_list = []\n",
    "\n",
    "    for i in range(0, 768, patch_size):  # 修改遍历顺序\n",
    "        for j in range(0, 1024, patch_size):\n",
    "            # 提取patch\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "\n",
    "            # 确保patch大小一致\n",
    "            if patch.shape[0] != patch_size or patch.shape[1] != patch_size:\n",
    "                patch = cv2.resize(patch, (patch_size, patch_size))\n",
    "\n",
    "            # 标准化并转换通道顺序\n",
    "            patch = patch / 255.0\n",
    "            patch = patch.transpose(2, 0, 1)  # (H,W,C) -> (C,H,W)\n",
    "            patches_list.append(patch)\n",
    "\n",
    "    # 转换为numpy数组，确保形状正确\n",
    "    patches_array = np.stack(patches_list)  # (N, C, H, W)\n",
    "\n",
    "    # 转换为tensor并移至设备\n",
    "    patches_tensor = torch.from_numpy(patches_array).float().to(device)\n",
    "\n",
    "    # 运行推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model(patches_tensor)\n",
    "        pred_masks = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "    # 重建完整mask\n",
    "    full_mask = np.zeros((768, 1024))\n",
    "    patch_idx = 0\n",
    "\n",
    "    for i in range(0, 768, patch_size):\n",
    "        for j in range(0, 1024, patch_size):\n",
    "            mask_patch = pred_masks[patch_idx, 0].cpu().numpy()\n",
    "\n",
    "            # 处理边界情况\n",
    "            h = min(patch_size, 768-i)\n",
    "            w = min(patch_size, 1024-j)\n",
    "\n",
    "            if mask_patch.shape != (h, w):\n",
    "                mask_patch = cv2.resize(mask_patch, (w, h))\n",
    "\n",
    "            full_mask[i:i+h, j:j+w] = mask_patch\n",
    "            patch_idx += 1\n",
    "\n",
    "    return full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = SegmentationDataset(train_data)\n",
    "test_dataset = SegmentationDataset(test_data)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SegmentationModel(num_classes=1).to(device)\n",
    "\n",
    "# # Train model\n",
    "# train_model(model, train_loader, test_loader, num_epochs=50, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/CompVis/ldm-super-resolution-4x-openimages1\n",
    "# Example inference\n",
    "test_image_path = test_data[0][\"image\"]\n",
    "# pred_mask = predict(model, test_image_path)\n",
    "\n",
    "# # Visualize results\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(cv2.imread(test_image_path))\n",
    "# plt.title('Original Image')\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(cv2.imread(test_data[0][\"annotation\"], cv2.IMREAD_GRAYSCALE))\n",
    "# plt.title('Ground Truth')\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(pred_mask)\n",
    "# plt.title('Prediction')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TimestepEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_channels, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保输入是浮点类型\n",
    "        x = x.float()\n",
    "        return self.layers(x)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_channels, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # 确保group数量能够整除通道数\n",
    "        groups = min(32, in_channels) # 动态调整groups数量\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            nn.GroupNorm(groups, in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.out_layers = nn.Sequential(\n",
    "            nn.GroupNorm(min(32, out_channels), out_channels), # 这里也要调整\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.skip_connection = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.skip_connection = nn.Identity()\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        h = self.in_layers(x)\n",
    "        emb_out = self.emb_layers(emb)[:, :, None, None]\n",
    "        h = h + emb_out\n",
    "        h = self.out_layers(h)\n",
    "        return h + self.skip_connection(x)\n",
    "\n",
    "class MemoryEfficientCrossAttention(nn.Module):\n",
    "    def __init__(self, dim, context_dim=None, heads=8, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim\n",
    "        context_dim = context_dim if context_dim is not None else dim\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        q = self.to_q(x)\n",
    "        context = context if context is not None else x\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "\n",
    "        return self.to_out(F.scaled_dot_product_attention(q, k, v))\n",
    "\n",
    "class BasicTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.attn1 = MemoryEfficientCrossAttention(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 8),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 8, dim)\n",
    "        )\n",
    "        self.attn2 = MemoryEfficientCrossAttention(dim, context_dim)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        x = self.attn1(self.norm1(x)) + x\n",
    "        x = self.attn2(self.norm2(x), context) + x\n",
    "        x = self.ff(self.norm3(x)) + x\n",
    "        return x\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self, in_channels, n_heads, d_head, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(32, in_channels)\n",
    "        inner_dim = n_heads * d_head\n",
    "        self.proj_in = nn.Linear(in_channels, inner_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            BasicTransformerBlock(inner_dim, context_dim)\n",
    "            for _ in range(1)\n",
    "        ])\n",
    "        self.proj_out = nn.Linear(inner_dim, in_channels)\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        b, c, h, w = x.shape\n",
    "        x_in = x\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 2, 3, 1).reshape(b, h*w, c)\n",
    "        x = self.proj_in(x)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, context)\n",
    "\n",
    "        x = self.proj_out(x)\n",
    "        x = x.reshape(b, h, w, c).permute(0, 3, 1, 2)\n",
    "        return x + x_in\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=4,\n",
    "        model_channels=320,\n",
    "        out_channels=4,\n",
    "        time_embed_dim=1280,\n",
    "        context_dim=2048\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_embed = TimestepEmbedding(model_channels, time_embed_dim)\n",
    "\n",
    "        # Input blocks\n",
    "        self.input_blocks = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, model_channels, 3, padding=1)\n",
    "        ])\n",
    "\n",
    "        # Add remaining input blocks\n",
    "        current_channels = model_channels\n",
    "        channel_multipliers = [1, 2, 4]  # 控制通道数增长\n",
    "\n",
    "        for i in range(len(channel_multipliers)):\n",
    "            out_channels = model_channels * channel_multipliers[i]\n",
    "            for _ in range(2):\n",
    "                layers = [ResBlock(current_channels, out_channels, time_embed_dim)]\n",
    "                if i % 3 == 2:\n",
    "                    layers.append(nn.Conv2d(out_channels, out_channels, 3, stride=2, padding=1))\n",
    "                if i > 0:\n",
    "                    layers.append(SpatialTransformer(out_channels, 8, 64, context_dim))\n",
    "                self.input_blocks.append(nn.Sequential(*layers))\n",
    "                current_channels = out_channels\n",
    "\n",
    "        # Middle block\n",
    "        self.middle_block = nn.Sequential(\n",
    "            ResBlock(current_channels, current_channels, time_embed_dim),\n",
    "            SpatialTransformer(current_channels, 8, 64, context_dim),\n",
    "            ResBlock(current_channels, current_channels, time_embed_dim)\n",
    "        )\n",
    "\n",
    "        # Output blocks\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for i in range(len(channel_multipliers)):\n",
    "            out_channels = model_channels * channel_multipliers[-(i+1)]\n",
    "            for _ in range(3):\n",
    "                layers = [ResBlock(current_channels, out_channels, time_embed_dim)]\n",
    "                if i > 0:\n",
    "                    layers.append(SpatialTransformer(out_channels, 8, 64, context_dim))\n",
    "                if i < len(channel_multipliers)-1:\n",
    "                    layers.append(nn.ConvTranspose2d(out_channels, out_channels, 4, 2, 1))\n",
    "                self.output_blocks.append(nn.Sequential(*layers))\n",
    "                current_channels = out_channels\n",
    "\n",
    "        # Final output\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(min(32, model_channels), model_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(model_channels, out_channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps, context):\n",
    "        # 确保timesteps是正确的形状和类型\n",
    "        if len(timesteps.shape) == 1:  # (batch_size,)\n",
    "            timesteps = timesteps.unsqueeze(1)  # (batch_size, 1)\n",
    "        timesteps = timesteps.float()\n",
    "\n",
    "        # Time embedding\n",
    "        temb = self.time_embed(timesteps)\n",
    "\n",
    "        # Input blocks\n",
    "        hs = []\n",
    "        h = x\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h) if not isinstance(module[0], ResBlock) else module[0](h, temb)\n",
    "            hs.append(h)\n",
    "\n",
    "        # Middle block\n",
    "        h = self.middle_block[0](h, temb)\n",
    "        h = self.middle_block[1](h)\n",
    "        h = self.middle_block[2](h, temb)\n",
    "\n",
    "        # Output blocks\n",
    "        for module in self.output_blocks:\n",
    "            h = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = module[0](h, temb)\n",
    "            if len(module) > 1:\n",
    "                if isinstance(module[1], SpatialTransformer):\n",
    "                    h = module[1](h)\n",
    "                    if len(module) > 2:\n",
    "                        h = module[2](h)\n",
    "                else:\n",
    "                    h = module[1](h)\n",
    "\n",
    "        return self.out(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialTrainUNet(UNetModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(in_channels=3, out_channels=1)  # 假设输入是RGB图像,输出是单通道mask\n",
    "\n",
    "        # 冻结前半部分参数\n",
    "        for param in self.input_blocks.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.middle_block.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Time embedding (我们不使用,所以传入零张量)\n",
    "        temb = torch.zeros(x.shape[0], 1280).to(x.device)  # 使用固定的 time_embed_dim\n",
    "\n",
    "        # Input blocks\n",
    "        hs = []\n",
    "        h = x\n",
    "        for module in self.input_blocks:\n",
    "            if isinstance(module, nn.Sequential):\n",
    "                if isinstance(module[0], ResBlock):\n",
    "                    h = module[0](h, temb)\n",
    "                    for layer in module[1:]:\n",
    "                        h = layer(h)\n",
    "                else:\n",
    "                    h = module(h)\n",
    "            else:\n",
    "                h = module(h)\n",
    "            hs.append(h)\n",
    "\n",
    "        # Middle block\n",
    "        if isinstance(self.middle_block, nn.Sequential):\n",
    "            for layer in self.middle_block:\n",
    "                if isinstance(layer, ResBlock):\n",
    "                    h = layer(h, temb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "        else:\n",
    "            h = self.middle_block(h)\n",
    "\n",
    "        # Output blocks (这部分参数是可训练的)\n",
    "        for module in self.output_blocks:\n",
    "            h = torch.cat([h, hs.pop()], dim=1)\n",
    "            if isinstance(module, nn.Sequential):\n",
    "                if isinstance(module[0], ResBlock):\n",
    "                    h = module[0](h, temb)\n",
    "                    for layer in module[1:]:\n",
    "                        if isinstance(layer, SpatialTransformer):\n",
    "                            h = layer(h)\n",
    "                        else:\n",
    "                            h = layer(h)\n",
    "                else:\n",
    "                    h = module(h)\n",
    "            else:\n",
    "                h = module(h)\n",
    "\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtins.safe_open' object has no attribute 'get_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_sdxl_unet.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(model_path, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# 这里我们假设文件中保存的状态字典为模型的状态字典\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensors\u001b[49m()\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 将模型设置为评估模式\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'builtins.safe_open' object has no attribute 'get_tensors'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors import safe_open\n",
    "\n",
    "# 创建模型实例\n",
    "model = UNetModel(in_channels=3, out_channels=1)  # 根据需要修改输入和输出通道\n",
    "\n",
    "# 加载权重\n",
    "model_path = \"output_sdxl_unet.safetensors\"\n",
    "\n",
    "with safe_open(model_path, framework=\"pt\") as f:\n",
    "    # 这里我们假设文件中保存的状态字典为模型的状态字典\n",
    "    state_dict = f.get_tensors()\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 如果需要，移动到 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m PartialTrainUNet()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 85\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[0;32m     82\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, PATCH_SIZE, PATCH_SIZE)\n\u001b[0;32m     84\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 85\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n\u001b[0;32m     87\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m, in \u001b[0;36mPartialTrainUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mSequential):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module[\u001b[38;5;241m0\u001b[39m], ResBlock):\n\u001b[1;32m---> 21\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m module[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     23\u001b[0m             h \u001b[38;5;241m=\u001b[39m layer(h)\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[1;34m(self, x, emb)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, emb):\n\u001b[1;32m---> 50\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     emb_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_layers(emb)[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     52\u001b[0m     h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m emb_out\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\python3.10\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PartialTrainUNet().to(device)\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_loader, test_loader, num_epochs=50, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
