{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是我对于Unet和attention_YNet的尝试，试着把Ynet用到的处理方式应用到Unet上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是第一部分,包含了:\n",
    "基础导入和工具函数\n",
    "图像处理类(填充/还原)\n",
    "数据集类\n",
    "数据加载器创建函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0000.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\0.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0001.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\1.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0002.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\2.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0003.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\3.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0004.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\4.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0005.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\5.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0006.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\6.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0007.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\7.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0008.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\8.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0009.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\9.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0010.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\10.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0011.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\11.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0012.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\12.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0013.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\13.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0014.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\14.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0015.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\15.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0016.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\16.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0017.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\17.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0018.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\18.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0019.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\19.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0020.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\20.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0021.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\21.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0022.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\22.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0023.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\23.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0024.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\24.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0025.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\25.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0026.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\26.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0027.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\27.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0028.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\28.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0029.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\29.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0030.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\30.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0031.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\31.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0032.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\32.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0033.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\33.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0034.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\34.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0035.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\35.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0036.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\36.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0037.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\37.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0038.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\38.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0039.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\39.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0040.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\40.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0041.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\41.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0042.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\42.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0043.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\43.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0044.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\44.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0045.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\45.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0046.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\46.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0047.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\47.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0048.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\48.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0049.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\49.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0050.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\50.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0051.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\51.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0052.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\52.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0053.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\53.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0054.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\54.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0055.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\55.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0056.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\56.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0057.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\57.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0058.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\58.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0059.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\59.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0060.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\60.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0061.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\61.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0062.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\62.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0063.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\63.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0064.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\64.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0065.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\65.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0066.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\66.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0067.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\67.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0068.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\68.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0069.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\69.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0070.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\70.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0071.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\71.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0072.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\72.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0073.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\73.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0074.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\74.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0075.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\75.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0076.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\76.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0077.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\77.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0078.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\78.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0079.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\79.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0080.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\80.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0081.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\81.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0082.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\82.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0083.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\83.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0084.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\84.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0085.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\85.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0086.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\86.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0087.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\87.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0088.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\88.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0089.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\89.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0090.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\90.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0091.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\91.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0092.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\92.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0093.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\93.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0094.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\94.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0095.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\95.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0096.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\96.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0097.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\97.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0098.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\98.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0099.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\99.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0100.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\100.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0101.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\101.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0102.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\102.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0103.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\103.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0104.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\104.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0105.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\105.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0106.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\106.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0107.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\107.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0108.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\108.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0109.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\109.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0110.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\110.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0111.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\111.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0112.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\112.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0113.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\113.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0114.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\114.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0115.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\115.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0116.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\116.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0117.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\117.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0118.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\118.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0119.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\119.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0120.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\120.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0121.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\121.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0122.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\122.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0123.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\123.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0124.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\124.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0125.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\125.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0126.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\126.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0127.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\127.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0128.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\128.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0129.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\129.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0130.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\130.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0131.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\131.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0132.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\132.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0133.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\133.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0134.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\134.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0135.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\135.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0136.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\136.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0137.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\137.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0138.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\138.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0139.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\139.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0140.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\140.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0141.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\141.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0142.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\142.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0143.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\143.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0144.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\144.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0145.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\145.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0146.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\146.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0147.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\147.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0148.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\148.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0149.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\149.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0150.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\150.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0151.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\151.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0152.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\152.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0153.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\153.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0154.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\154.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0155.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\155.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0156.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\156.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0157.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\157.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0158.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\158.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0159.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\159.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0160.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\160.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0161.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\161.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0162.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\162.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0163.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\163.png'}, {'image': 'dataset\\\\Lucchi++\\\\Train_In\\\\mask0164.png', 'annotation': 'dataset\\\\Lucchi++\\\\Train_Out\\\\164.png'}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "# Path to the chest-ct-segmentation dataset folder\n",
    "data_dir = \"dataset\\Lucchi++\" \n",
    "train_images_dir = os.path.join(data_dir, \"Train_In\")\n",
    "train_masks_dir = os.path.join(data_dir, \"Train_Out\")\n",
    "test_images_dir = os.path.join(data_dir, \"Test_In\")\n",
    "test_masks_dir = os.path.join(data_dir, \"Test_Out\")\n",
    "PATCH_SIZE = 128\n",
    "\n",
    "i = 0\n",
    "# Prepare the training data, Append image and corresponding mask paths\n",
    "train_data = []\n",
    "for image_file in os.listdir(train_images_dir):\n",
    "    image_path = os.path.join(train_images_dir, image_file)\n",
    "    mask_path = os.path.join(train_masks_dir, f\"{i}.png\")\n",
    "    i += 1\n",
    "    train_data.append(\n",
    "    { \n",
    "        \"image\" : image_path, \n",
    "        \"annotation\" : mask_path\n",
    "    })\n",
    "\n",
    "i = 0\n",
    "# Prepare the test data, Append image and corresponding mask paths\n",
    "test_data = []\n",
    "for image_file in os.listdir(test_images_dir):\n",
    "    image_path = os.path.join(test_images_dir, image_file)\n",
    "    mask_path = os.path.join(test_masks_dir, f\"{i}.png\")\n",
    "    i += 1\n",
    "    test_data.append(\n",
    "    { \n",
    "        \"image\" : image_path, \n",
    "        \"annotation\" : mask_path\n",
    "    })\n",
    "print(train_data)\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.conv_block(3, 32, stride=2),\n",
    "            self.conv_block(32, 64, stride=2),\n",
    "            self.conv_block(64, 128, stride=2),\n",
    "            self.conv_block(128, 256, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upconv_block(256, 128),\n",
    "            self.upconv_block(128, 64),\n",
    "            self.upconv_block(64, 32),\n",
    "            self.upconv_block(32, 32)\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.final = nn.Conv2d(32, num_classes, kernel_size=3, padding=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels, stride=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        features = []\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "            features.append(x)\n",
    "        \n",
    "        # Decoder\n",
    "        for i, decoder_layer in enumerate(self.decoder):\n",
    "            x = decoder_layer(x)\n",
    "            if i < len(self.decoder) - 1:\n",
    "                x = x + features[-i-2]  # Skip connection\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "# Custom Dataset class\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list, patch_size=128, stride=64, transform=None):\n",
    "        self.data_list = data_list\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 预处理所有图像的patches\n",
    "        self.all_patches = []\n",
    "        self.all_masks = []\n",
    "        self.all_positions = []\n",
    "        self.all_sizes = []\n",
    "        self.all_paths = []\n",
    "        \n",
    "        for item in data_list:\n",
    "            image = cv2.imread(item[\"image\"])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            mask = cv2.imread(item[\"annotation\"], cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            patches, mask_patches, positions, original_size = self.process_image_to_patches(image, mask)\n",
    "            \n",
    "            for patch, mask_patch in zip(patches, mask_patches):\n",
    "                if self.transform:\n",
    "                    patch = self.transform(patch)\n",
    "                \n",
    "                patch = torch.FloatTensor(patch.transpose(2, 0, 1)) / 255.0\n",
    "                mask_patch = torch.FloatTensor(mask_patch).unsqueeze(0) / 255.0\n",
    "                \n",
    "                self.all_patches.append(patch)\n",
    "                self.all_masks.append(mask_patch)\n",
    "                self.all_positions.append(positions)\n",
    "                self.all_sizes.append(original_size)\n",
    "                self.all_paths.append(item[\"image\"])\n",
    "\n",
    "    def process_image_to_patches(self, image, mask):\n",
    "        \"\"\"处理图像和掩码为patches\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        patches = []\n",
    "        mask_patches = []\n",
    "        positions = []\n",
    "        \n",
    "        for y in range(0, h-self.patch_size+1, self.stride):\n",
    "            for x in range(0, w-self.patch_size+1, self.stride):\n",
    "                patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                \n",
    "                patches.append(patch)\n",
    "                mask_patches.append(mask_patch)\n",
    "                positions.append((y, x))\n",
    "        \n",
    "        # 处理边缘情况\n",
    "        if h % self.stride != 0:\n",
    "            y = h - self.patch_size\n",
    "            for x in range(0, w-self.patch_size+1, self.stride):\n",
    "                patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                patches.append(patch)\n",
    "                mask_patches.append(mask_patch)\n",
    "                positions.append((y, x))\n",
    "        \n",
    "        if w % self.stride != 0:\n",
    "            x = w - self.patch_size\n",
    "            for y in range(0, h-self.patch_size+1, self.stride):\n",
    "                patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                patches.append(patch)\n",
    "                mask_patches.append(mask_patch)\n",
    "                positions.append((y, x))\n",
    "        \n",
    "        if h % self.stride != 0 and w % self.stride != 0:\n",
    "            y = h - self.patch_size\n",
    "            x = w - self.patch_size\n",
    "            patch = image[y:y+self.patch_size, x:x+self.patch_size]\n",
    "            mask_patch = mask[y:y+self.patch_size, x:x+self.patch_size]\n",
    "            patches.append(patch)\n",
    "            mask_patches.append(mask_patch)\n",
    "            positions.append((y, x))\n",
    "        \n",
    "        return patches, mask_patches, positions, (h, w)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'patches': self.all_patches[idx],\n",
    "            'mask_patches': self.all_masks[idx],\n",
    "            'positions': self.all_positions[idx],\n",
    "            'original_size': self.all_sizes[idx],\n",
    "            'image_path': self.all_paths[idx]\n",
    "        } \n",
    "    \n",
    "def calculate_metrics(pred_mask, true_mask, threshold=0.5):\n",
    "    # Convert predictions to binary\n",
    "    pred_mask = (pred_mask > threshold).float()\n",
    "\n",
    "    # Calculate intersection and union\n",
    "    intersection = (pred_mask * true_mask).sum()\n",
    "    union = pred_mask.sum() + true_mask.sum() - intersection\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = (intersection + 1e-7) / (union + 1e-7)\n",
    "\n",
    "    # Calculate Dice coefficient\n",
    "    dice = (2. * intersection + 1e-7) / (pred_mask.sum() + true_mask.sum() + 1e-7)\n",
    "\n",
    "    return iou.item(), dice.item()\n",
    "\n",
    "# Add reconstruction function\n",
    "def reconstruct_from_patches(patches, positions, original_size, patch_size, stride):\n",
    "    \"\"\"从patches重建完整图像\"\"\"\n",
    "    h, w = original_size\n",
    "    reconstructed = np.zeros((h, w), dtype=np.float32)\n",
    "    count = np.zeros((h, w), dtype=np.float32)\n",
    "    \n",
    "    for patch, (y, x) in zip(patches, positions):\n",
    "        patch_h = min(patch_size, h - y)\n",
    "        patch_w = min(patch_size, w - x)\n",
    "        reconstructed[y:y+patch_h, x:x+patch_w] += patch[:patch_h, :patch_w]\n",
    "        count[y:y+patch_h, x:x+patch_w] += 1\n",
    "    \n",
    "    # 处理重叠区域\n",
    "    count[count == 0] = 1\n",
    "    reconstructed /= count\n",
    "    return reconstructed\n",
    "\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, device=\"cuda\"):\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_val_iou = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_dice = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader):\n",
    "            patches = batch['patches'].to(device)\n",
    "            mask_patches = batch['mask_patches'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(patches)\n",
    "            loss = criterion(outputs, mask_patches)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pred_masks = torch.sigmoid(outputs) > 0.5\n",
    "            iou, dice = calculate_metrics(pred_masks.float(), mask_patches)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_iou += iou\n",
    "            train_dice += dice\n",
    "        \n",
    "        # 计算平均值\n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_dice /= len(train_loader)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "        val_dice = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                patches = batch['patches'].to(device)\n",
    "                mask_patches = batch['mask_patches'].to(device)\n",
    "                \n",
    "                outputs = model(patches)\n",
    "                loss = criterion(outputs, mask_patches)\n",
    "                \n",
    "                pred_masks = torch.sigmoid(outputs) > 0.5\n",
    "                iou, dice = calculate_metrics(pred_masks.float(), mask_patches)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_iou += iou\n",
    "                val_dice += dice\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_iou /= len(val_loader)\n",
    "        val_dice /= len(val_loader)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, IoU: {train_iou:.4f}, Dice: {train_dice:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, IoU: {val_iou:.4f}, Dice: {val_dice:.4f}')\n",
    "        \n",
    "        if val_iou > best_val_iou:\n",
    "            best_val_iou = val_iou\n",
    "            torch.save(model.state_dict(), 'best_model_iou.pth')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_loss.pth')\n",
    "# Inference function\n",
    "def predict(model, image_path, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (1024, 768))\n",
    "    \n",
    "    # Parameters for overlapping patches\n",
    "    patch_size = PATCH_SIZE\n",
    "    overlap = patch_size // 2  # 50% overlap\n",
    "    \n",
    "    # Calculate steps with overlap\n",
    "    h_steps = int(np.ceil((768 - patch_size) / (patch_size - overlap))) + 1\n",
    "    w_steps = int(np.ceil((1024 - patch_size) / (patch_size - overlap))) + 1\n",
    "    \n",
    "    patches_list = []\n",
    "    patch_positions = []  # Store positions for reconstruction\n",
    "    \n",
    "    # Extract overlapping patches\n",
    "    for i in range(h_steps):\n",
    "        for j in range(w_steps):\n",
    "            # Calculate patch coordinates\n",
    "            y_start = min(i * (patch_size - overlap), 768 - patch_size)\n",
    "            x_start = min(j * (patch_size - overlap), 1024 - patch_size)\n",
    "            \n",
    "            # Extract patch\n",
    "            patch = image[y_start:y_start+patch_size, x_start:x_start+patch_size]\n",
    "            \n",
    "            # Normalize and convert channel order\n",
    "            patch = patch / 255.0\n",
    "            patch = patch.transpose(2, 0, 1)\n",
    "            patches_list.append(patch)\n",
    "            patch_positions.append((y_start, x_start))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    patches_array = np.stack(patches_list)\n",
    "    patches_tensor = torch.from_numpy(patches_array).float().to(device)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(patches_tensor)\n",
    "        pred_masks = torch.sigmoid(outputs) > 0.5\n",
    "    \n",
    "    # Initialize weight and prediction maps\n",
    "    weight_map = np.zeros((768, 1024))\n",
    "    pred_map = np.zeros((768, 1024))\n",
    "    \n",
    "    # Create weight kernel for blending\n",
    "    y, x = np.mgrid[0:patch_size, 0:patch_size]\n",
    "    weight_kernel = np.exp(-((x - patch_size/2)**2 + (y - patch_size/2)**2) / (2*(patch_size/4)**2))\n",
    "    \n",
    "    # Reconstruct full mask with weighted averaging\n",
    "    for idx, (y_start, x_start) in enumerate(patch_positions):\n",
    "        mask_patch = pred_masks[idx, 0].cpu().numpy()\n",
    "        \n",
    "        # Apply weight kernel\n",
    "        weighted_patch = mask_patch * weight_kernel\n",
    "        \n",
    "        # Add to prediction and weight maps\n",
    "        y_end = min(y_start + patch_size, 768)\n",
    "        x_end = min(x_start + patch_size, 1024)\n",
    "        h, w = y_end - y_start, x_end - x_start\n",
    "        \n",
    "        pred_map[y_start:y_end, x_start:x_end] += weighted_patch[:h, :w]\n",
    "        weight_map[y_start:y_end, x_start:x_end] += weight_kernel[:h, :w]\n",
    "    \n",
    "    # Normalize by weights\n",
    "    full_mask = np.divide(pred_map, weight_map, where=weight_map > 0)\n",
    "    full_mask = (full_mask > 0.5).astype(np.float32)\n",
    "    \n",
    "    return full_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1702/1702 [21:33<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:\n",
      "Train Loss: 0.0931, IoU: 0.7630, Dice: 0.8512\n",
      "Val Loss: 0.0560, IoU: 0.7202, Dice: 0.8242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1702/1702 [30:43<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:\n",
      "Train Loss: 0.0175, IoU: 0.9148, Dice: 0.9547\n",
      "Val Loss: 0.0529, IoU: 0.7448, Dice: 0.8404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1702/1702 [30:27<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50:\n",
      "Train Loss: 0.0120, IoU: 0.9370, Dice: 0.9672\n",
      "Val Loss: 0.0635, IoU: 0.7372, Dice: 0.8368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 60/1702 [01:01<28:07,  1.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m SegmentationModel(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 250\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, device)\u001b[0m\n\u001b[0;32m    247\u001b[0m train_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    248\u001b[0m train_dice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m    251\u001b[0m     patches \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatches\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    252\u001b[0m     mask_patches \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_patches\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m--> 171\u001b[0m         \u001b[43m{\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    171\u001b[0m         {\n\u001b[1;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[0;32m    176\u001b[0m         }\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = SegmentationDataset(train_data)\n",
    "test_dataset = SegmentationDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SegmentationModel(num_classes=1).to(device)\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_loader, test_loader, num_epochs=50, device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的测试： train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "patch_size=128, stride=64   \n",
    "\n",
    "\n",
    "100%|██████████| 1702/1702 [21:33<00:00,  1.32it/s]\n",
    "Epoch 1/50:\n",
    "Train Loss: 0.0931, IoU: 0.7630, Dice: 0.8512\n",
    "Val Loss: 0.0560, IoU: 0.7202, Dice: 0.8242\n",
    "100%|██████████| 1702/1702 [30:43<00:00,  1.08s/it]\n",
    "Epoch 2/50:\n",
    "Train Loss: 0.0175, IoU: 0.9148, Dice: 0.9547\n",
    "Val Loss: 0.0529, IoU: 0.7448, Dice: 0.8404\n",
    "100%|██████████| 1702/1702 [30:27<00:00,  1.07s/it]\n",
    "Epoch 3/50:\n",
    "Train Loss: 0.0120, IoU: 0.9370, Dice: 0.9672\n",
    "Val Loss: 0.0635, IoU: 0.7372, Dice: 0.8368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/CompVis/ldm-super-resolution-4x-openimages1\n",
    "# Example inference\n",
    "test_image_path = test_data[0][\"image\"]\n",
    "pred_mask = predict(model, test_image_path)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(cv2.imread(test_image_path))\n",
    "plt.title('Original Image')\n",
    "plt.subplot(132)\n",
    "plt.imshow(cv2.imread(test_data[0][\"annotation\"], cv2.IMREAD_GRAYSCALE))\n",
    "plt.title('Ground Truth')\n",
    "plt.subplot(133)\n",
    "plt.imshow(pred_mask)\n",
    "plt.title('Prediction')\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"overlaping_unet_segmentation_try.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load from oxford_segmentation_models.pth\n",
    "import torch\n",
    "\n",
    "def load_model(model_path, num_classes, device):\n",
    "    # Create an instance of your model\n",
    "    model = SegmentationModel(num_classes)\n",
    "    \n",
    "    # Load the state dict\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "     \n",
    "    # Load the state dict into your model\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Move the model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 1 \n",
    "model_path = \"simple_unet_segmentation.pth\"\n",
    "\n",
    "model = load_model(model_path, num_classes, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
