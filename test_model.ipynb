{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载Lucchi++的函数设定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集大小: 165\n",
      "图像形状: (768, 1024, 3)\n",
      "掩码形状: (1, 768, 1024)\n",
      "点标注形状: (21, 1, 2)\n",
      "掩码数量: 21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "class LucchiPPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lucchi++数据集加载器\n",
    "    数据集结构：\n",
    "    dataset/Lucchi++/\n",
    "        ├── Train_In/    \n",
    "        ├── Train_Out/   \n",
    "        ├── Test_In/     \n",
    "        └── Test_Out/    \n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, split='test', transform=None):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            data_dir (str): Lucchi++数据集的根目录\n",
    "            split (str): 'train' 或 'test'\n",
    "            transform: 可选的图像变换\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 设置图像和掩码目录\n",
    "        if split == 'train':\n",
    "            self.image_dir = os.path.join(data_dir, \"Train_In\")\n",
    "            self.mask_dir = os.path.join(data_dir, \"Train_Out\")\n",
    "        else:\n",
    "            self.image_dir = os.path.join(data_dir, \"Test_In\")\n",
    "            self.mask_dir = os.path.join(data_dir, \"Test_Out\")\n",
    "            \n",
    "        # 获取所有图像文件\n",
    "        self.image_files = sorted(os.listdir(self.image_dir))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图像路径\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, f\"{idx}.png\")\n",
    "        \n",
    "        # 读取图像和掩码\n",
    "        image = cv2.imread(image_path)[..., ::-1]  # BGR转RGB\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if image is None or mask is None:\n",
    "            raise ValueError(f\"无法读取图像或掩码: {image_path}, {mask_path}\")\n",
    "            \n",
    "        # 调整大小\n",
    "        r = min(1024 / image.shape[1], 1024 / image.shape[0])\n",
    "        image = cv2.resize(image, (int(image.shape[1] * r), int(image.shape[0] * r)))\n",
    "        mask = cv2.resize(mask, (int(mask.shape[1] * r), int(mask.shape[0] * r)), \n",
    "                         interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # 二值化掩码\n",
    "        binary_mask = (mask > 0).astype(np.uint8)\n",
    "        \n",
    "        # 获取点标注\n",
    "        eroded_mask = cv2.erode(binary_mask, np.ones((5, 5), np.uint8), iterations=1)\n",
    "        labels = measure.label(eroded_mask)\n",
    "        regions = measure.regionprops(labels)\n",
    "        \n",
    "        points = []\n",
    "        for region in regions:\n",
    "            y, x = region.coords[np.random.randint(len(region.coords))]\n",
    "            points.append([x, y])\n",
    "            \n",
    "        points = np.array(points)\n",
    "        \n",
    "        # 调整维度\n",
    "        binary_mask = np.expand_dims(binary_mask, axis=0)  # (1, H, W)\n",
    "        if len(points) > 0:\n",
    "            points = np.expand_dims(points, axis=1)  # (N, 1, 2)\n",
    "            \n",
    "        num_masks = len(regions)\n",
    "        \n",
    "        return image, binary_mask, points, num_masks\n",
    "\n",
    "def load_lucchi_dataset(data_dir=\"dataset/Lucchi++\", split='test'):\n",
    "    \"\"\"\n",
    "    加载Lucchi++数据集\n",
    "    \n",
    "    参数:\n",
    "        data_dir (str): 数据集根目录\n",
    "        split (str): 'train' 或 'test'\n",
    "    \n",
    "    返回:\n",
    "        LucchiPPDataset对象\n",
    "    \"\"\"\n",
    "    return LucchiPPDataset(data_dir, split)\n",
    "\n",
    "# 使用示例：\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载测试集\n",
    "    test_dataset = load_lucchi_dataset(data_dir=\"dataset/Lucchi++\", split='test')\n",
    "    \n",
    "    # 查看数据集大小\n",
    "    print(f\"测试集大小: {len(test_dataset)}\")\n",
    "    \n",
    "    # 获取一个样本\n",
    "    image, mask, points, num_masks = test_dataset[0]\n",
    "    \n",
    "    # 打印形状\n",
    "    print(f\"图像形状: {image.shape}\")\n",
    "    print(f\"掩码形状: {mask.shape}\")\n",
    "    print(f\"点标注形状: {points.shape if points.size > 0 else 'No points'}\")\n",
    "    print(f\"掩码数量: {num_masks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "def evaluate_model(predictor, test_dataset, save_dir=None):\n",
    "    \"\"\"\n",
    "    评估模型在测试数据集上的性能\n",
    "    \n",
    "    Args:\n",
    "        predictor: 加载了预训练模型的SAM2预测器\n",
    "        test_dataset: 测试数据集\n",
    "        save_dir: 可选，保存预测结果的目录\n",
    "    \"\"\"\n",
    "    predictor.model.eval()  # 设置为评估模式\n",
    "    all_ious = []\n",
    "    \n",
    "    # 创建保存目录\n",
    "    if save_dir and not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        for idx, (image, gt_mask, input_point, num_masks) in enumerate(tqdm(test_dataset)):\n",
    "            if image is None or gt_mask is None or num_masks == 0:\n",
    "                continue\n",
    "                \n",
    "            # 准备输入数据\n",
    "            input_point = np.array(input_point)\n",
    "            input_label = np.ones((num_masks, 1))\n",
    "            \n",
    "            # 基本的数据检查\n",
    "            if not isinstance(input_point, np.ndarray) or not isinstance(input_label, np.ndarray):\n",
    "                continue\n",
    "            if input_point.size == 0 or input_label.size == 0:\n",
    "                continue\n",
    "                \n",
    "            # 设置图像并获取预测\n",
    "            predictor.set_image(image)\n",
    "            pred_masks = predictor.predict(\n",
    "                point_coords=input_point,\n",
    "                point_labels=input_label\n",
    "            )\n",
    "            \n",
    "            # 计算IoU\n",
    "            gt_mask = torch.tensor(gt_mask.astype(np.float32)).cuda()\n",
    "            pred_mask = torch.tensor(pred_masks > 0.5).cuda().float()\n",
    "            \n",
    "            intersection = (gt_mask * pred_mask).sum((1, 2))\n",
    "            union = gt_mask.sum((1, 2)) + pred_mask.sum((1, 2)) - intersection\n",
    "            iou = (intersection / (union + 1e-6)).cpu().numpy()\n",
    "            \n",
    "            all_ious.extend(iou)\n",
    "            \n",
    "            # 可选：保存预测结果\n",
    "            if save_dir:\n",
    "                save_path = os.path.join(save_dir, f'pred_{idx}.png')\n",
    "                # 这里添加保存预测mask的代码\n",
    "                \n",
    "    # 计算统计信息\n",
    "    mean_iou = np.mean(all_ious)\n",
    "    std_iou = np.std(all_ious)\n",
    "    \n",
    "    return mean_iou, std_iou\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Lucchi+ dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/165 [00:00<?, ?it/s]e:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\hieradet.py:70: UserWarning: Memory efficient kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:778.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "e:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\hieradet.py:70: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:558.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "e:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\hieradet.py:70: UserWarning: Flash attention kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:780.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "e:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\hieradet.py:70: UserWarning: Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:604.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "e:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\hieradet.py:70: UserWarning: CuDNN attention kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:782.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "e:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\hieradet.py:70: UserWarning: Expected query, key and value to all be of dtype: {Half, BFloat16}. Got Query dtype: float, Key dtype: float, and Value dtype: float instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:110.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "  0%|          | 0/165 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No available kernel. Aborting execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m±\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 42\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m mean_iou, std_iou \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m results[dataset_name] \u001b[38;5;241m=\u001b[39m (mean_iou, std_iou)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Results: IoU = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m±\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(predictor, test_dataset, save_dir)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 设置图像并获取预测\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m pred_masks \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     44\u001b[0m     point_coords\u001b[38;5;241m=\u001b[39minput_point,\n\u001b[0;32m     45\u001b[0m     point_labels\u001b[38;5;241m=\u001b[39minput_label\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 计算IoU\u001b[39;00m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\GitHubDesktopPath\\sam2\\sam2\\sam2_image_predictor.py:117\u001b[0m, in \u001b[0;36mSAM2ImagePredictor.set_image\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mlen\u001b[39m(input_image\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m input_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    115\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_image must be of size 1x3xHxW, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_image\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing image embeddings for the provided image...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m backbone_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m _, vision_feats, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_prepare_backbone_features(backbone_out)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Add no_mem_embed, which is added to the lowest rest feat. map during training on videos\u001b[39;00m\n",
      "File \u001b[1;32me:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\sam2_base.py:469\u001b[0m, in \u001b[0;36mSAM2Base.forward_image\u001b[1;34m(self, img_batch)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_batch: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    468\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the image feature on the input batch.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m     backbone_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_high_res_features_in_sam:\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;66;03m# precompute projected level 0 and level 1 features in SAM decoder\u001b[39;00m\n\u001b[0;32m    472\u001b[0m         \u001b[38;5;66;03m# to avoid running it again on every SAM click\u001b[39;00m\n\u001b[0;32m    473\u001b[0m         backbone_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone_fpn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msam_mask_decoder\u001b[38;5;241m.\u001b[39mconv_s0(\n\u001b[0;32m    474\u001b[0m             backbone_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone_fpn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    475\u001b[0m         )\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\image_encoder.py:31\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Forward through backbone\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     features, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m# Discard the lowest resolution features\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         features, pos \u001b[38;5;241m=\u001b[39m features[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp], pos[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalp]\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\hieradet.py:292\u001b[0m, in \u001b[0;36mHiera.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    290\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m--> 292\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_ends[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    294\u001b[0m         i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_ends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_interm_layers\n\u001b[0;32m    295\u001b[0m     ):\n\u001b[0;32m    296\u001b[0m         feats \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\hieradet.py:149\u001b[0m, in \u001b[0;36mMultiScaleBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    146\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, window_size)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Window Attention + Q Pooling (if stage change)\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_stride:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# Shapes have changed due to Q pooling\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_stride[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\GitHubDesktopPath\\sam2\\sam2\\modeling\\backbones\\hieradet.py:70\u001b[0m, in \u001b[0;36mMultiScaleAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m     q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mreshape(B, H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Torch's SDPA expects [B, nheads, H*W, C] so we transpose\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Transpose back\u001b[39;00m\n\u001b[0;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No available kernel. Aborting execution."
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # 设置设备\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 加载预训练模型\n",
    "    sam2_checkpoint = \"sam2_lora_checkpoint_3000.pth\"  # 模型路径\n",
    "    model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"  # 配置文件路径\n",
    "    \n",
    "    # 构建模型\n",
    "    model = build_sam2(model_cfg)  # 假设 build_sam2 返回模型\n",
    "    \n",
    "    # 加载权重\n",
    "    checkpoint = torch.load(sam2_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint, strict=False)  # 加载权重\n",
    "    model = model.to(device)  # 将模型迁移到设备\n",
    "    \n",
    "    # 初始化预测器\n",
    "    predictor = SAM2ImagePredictor(model)\n",
    "    \n",
    "    # 加载测试数据集\n",
    "    test_datasets = {\n",
    "        'Lucchi+': load_lucchi_dataset(),  # 需要实现\n",
    "        #  'VNC': load_vnc_dataset()          \n",
    "    }\n",
    "    \n",
    "    # 在每个测试数据集上评估\n",
    "    results = {}\n",
    "    for dataset_name, dataset in test_datasets.items():\n",
    "        print(f\"\\nEvaluating on {dataset_name} dataset...\")\n",
    "        \n",
    "        # 创建保存目录\n",
    "        save_dir = os.path.join(\"results\", dataset_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # 评估模型\n",
    "        mean_iou, std_iou = evaluate_model(\n",
    "            predictor=predictor,\n",
    "            test_dataset=dataset,\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "        \n",
    "        results[dataset_name] = (mean_iou, std_iou)\n",
    "        print(f\"{dataset_name} Results: IoU = {mean_iou:.3f}±{std_iou:.3f}\")\n",
    "    \n",
    "    # 打印总结果\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for dataset_name, (mean_iou, std_iou) in results.items():\n",
    "        print(f\"{dataset_name}: {mean_iou:.3f}±{std_iou:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sam_mask_decoder.transformer.layers.0.self_attn.q_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.lora_up.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.lora_down.weight', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.lora_up.weight', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.lora_down.weight', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.lora_up.weight', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.lora_down.weight', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.lora_up.weight', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.lora_down.weight', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.lora_up.weight', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.lora_down.weight', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.lora_up.weight', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.lora_down.weight', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.lora_up.weight', 'sam_mask_decoder.iou_prediction_head.layers.0.lora_down.weight', 'sam_mask_decoder.iou_prediction_head.layers.0.lora_up.weight', 'sam_mask_decoder.iou_prediction_head.layers.1.lora_down.weight', 'sam_mask_decoder.iou_prediction_head.layers.1.lora_up.weight', 'sam_mask_decoder.iou_prediction_head.layers.2.lora_down.weight', 'sam_mask_decoder.iou_prediction_head.layers.2.lora_up.weight', 'sam_mask_decoder.pred_obj_score_head.layers.0.lora_down.weight', 'sam_mask_decoder.pred_obj_score_head.layers.0.lora_up.weight', 'sam_mask_decoder.pred_obj_score_head.layers.1.lora_down.weight', 'sam_mask_decoder.pred_obj_score_head.layers.1.lora_up.weight', 'sam_mask_decoder.pred_obj_score_head.layers.2.lora_down.weight', 'sam_mask_decoder.pred_obj_score_head.layers.2.lora_up.weight'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load(\"sam2_lora_checkpoint_3000.pth\", map_location=\"cuda\")\n",
    "print(checkpoint.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
